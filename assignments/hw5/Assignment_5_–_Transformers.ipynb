{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eXXcHxn1QsQ"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "Text classification is one of the tasks that is addressed in natural language processing (NLP).  Like with computer vision, NLP uses deep learning.  A particular kind of deep learning model that is used in NLP is called the transformer.  If you're interested in learning  about transformers in this [blog post](http://jalammar.github.io/illustrated-transformer/).  We'll be using an implementation of transformers from an open source package called Hugging Face.  \n",
        "\n",
        "For this assignment, we'll look at wallk through a text classification task called *intent detection*.  When you talk to your Amazon Alexa, it needs to figure out what you're trying to do.  Are you trying to play music?  Do you want to check the weather?  Are you setting a timer?  Are you trying to get a recipe to cook something?  Depending on what it thinks your intent is, it routes your message to a specialized module to handle your request.\n",
        "\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "Run the following cells to download the dataset and preprocess it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL3aHx-Pp-NW",
        "cellView": "form"
      },
      "source": [
        "#@title Run me and restart runtime environment!\n",
        "###################### Restart Runtime after executing this cell ######################################################\n",
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0pqZc3Gs5xP",
        "cellView": "form"
      },
      "source": [
        "#@title Run me! – Data Preprocessing 1 \n",
        "###**Do not modify the code in the cells below (I am planning to remove some of the below cells used for preperocessing and subsettng the data and provide the final csv to the students instead)**\n",
        "### gathering the dataset\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "SNIPS_DATA_BASE_URL = (\n",
        "    \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n",
        "    \"master/data/snips/\"\n",
        ")\n",
        "for filename in [\"train\", \"valid\", \"vocab.intent\"]:\n",
        "    path = Path(filename)\n",
        "    if not path.exists():\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UFdRUzGs8Qa"
      },
      "source": [
        "#@title Run me! – Data Preprocessing 2\r\n",
        "### gathering the dataset\r\n",
        "def parse_line(line):\r\n",
        "    utterance_data, intent_label = line.split(\" <=> \")\r\n",
        "    items = utterance_data.split()\r\n",
        "    words = [item.rsplit(\":\", 1)[0]for item in items]\r\n",
        "    word_labels = [item.rsplit(\":\", 1)[1]for item in items]\r\n",
        "    return {\r\n",
        "        \"intent_label\": intent_label,\r\n",
        "        \"words\": \" \".join(words),\r\n",
        "        \"word_labels\": \" \".join(word_labels),\r\n",
        "        \"length\": len(words),\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6TXM3M-D0q"
      },
      "source": [
        "## Intent Classification Dataset \n",
        "\n",
        "In this part of the assignment, we will be using a subset of [intent classifcation dataset \n",
        "](https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips). We will be classifying a given sentence into one of the two categories:\n",
        "AddToPlaylist and \n",
        "PlayMusic \n",
        "\n",
        "We will also provided a separate validation set to test the performance of the model. In the next cells, the data will be cleaned for you to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T0AHZcHytAYG",
        "outputId": "36c10d49-7d4a-4404-82c4-6877b6a2e92a"
      },
      "source": [
        "#@title Run me! – Data Cleaning & First 5 Rows of Dataset\n",
        "################ data cleaning #############\n",
        "intent_names = Path(\"vocab.intent\").read_text().split()\n",
        "intent_names=intent_names[0:1]+intent_names[3:4]\n",
        "intent_map = dict((label, idx) for idx, label in enumerate(intent_names))\n",
        "\n",
        "lines_train = Path(\"train\").read_text().strip().splitlines()\n",
        "\n",
        "parsed = [parse_line(line) for line in lines_train]\n",
        "\n",
        "df_train = pd.DataFrame([p for p in parsed if p is not None])\n",
        "\n",
        "## getting a subset of data\n",
        "count=500 \n",
        "#df_train_small= pd.concat([df_train[0:0+count],df_train[1842:1842+count],df_train[3715:3715+count],df_train[5615:5615+count],df_train[7515:7515+count],df_train[9371:9371+count],df_train[11225:11225+count]]) \n",
        "\n",
        "df_train_= pd.concat([df_train[0:0+count],df_train[5615:5615+count]]) \n",
        "\n",
        "df_train_=df_train_[['words','intent_label']]\n",
        "\n",
        "df_train_.replace({\"intent_label\":intent_map },inplace=True)\n",
        "\n",
        "df_train_=df_train_.sample(frac=1,random_state=56132).reset_index(drop=True)\n",
        "\n",
        "df_train_.head()\n",
        "\n",
        "\n",
        "## introducing noise\n",
        "df_1=df_train_.sample(frac=0.3,random_state=111)\n",
        "\n",
        "df_0=df_train_.sample(frac=0.3,random_state=80)\n",
        "\n",
        "df_train_.loc[df_0.index ,'intent_label'] = 0\n",
        "df_train_.loc[df_1.index ,'intent_label'] = 1\n",
        "\n",
        "\n",
        "df_train_['intent_label'].value_counts()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>intent_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Play a song off Ian Stuart Donaldson 's Nature...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>add an album to playlist Emily Dickinson</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Shawnna to the Warm Hearts Feel Good playlist.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Play some music using slacker</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Can you play Maggie Mae on Netflix</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               words  intent_label\n",
              "0  Play a song off Ian Stuart Donaldson 's Nature...             1\n",
              "1           add an album to playlist Emily Dickinson             0\n",
              "2   A Shawnna to the Warm Hearts Feel Good playlist.             0\n",
              "3                      Play some music using slacker             1\n",
              "4                 Can you play Maggie Mae on Netflix             1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdqBXyvao5Oo"
      },
      "source": [
        "Awesome, we're all set to get started with the actual analysis! In the next step, we will split the training and test data using the sklearn train_test_split function (see how it works [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). To decrease the training time (our current dataset has 1000 rows!), we will create a smaller subset of the training data called df_train_small. Note here that the parameter 'train_size' and 'test_size' indicate the proportion of data that is used for the training/testing dataset respectively. More information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-psMCyhGgfT"
      },
      "source": [
        " from sklearn.model_selection import train_test_split  \n",
        "\n",
        "def subset_datafarme(train_size):       \n",
        "        X=df_train_['words']\n",
        "        y=df_train_['intent_label']\n",
        "        X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=train_size, stratify=y,random_state=444)\n",
        "        df_train_small=pd.DataFrame({'words':X_train,'intent_label':y_train})\n",
        "\n",
        "        return df_train_small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsx3nJm1rAni"
      },
      "source": [
        "By running the following cell, you can see the intent map, i.e. which intent is mapped to which label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUr-EJJSCCfP",
        "outputId": "02ccaaef-1510-4480-b15e-9a5ecd08bff2"
      },
      "source": [
        "intent_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AddToPlaylist': 0, 'PlayMusic': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAmErXHyuc8w",
        "cellView": "form"
      },
      "source": [
        "#@title Run me! – Validation Set Cleaning & First 5 Rows of Dataset\n",
        "\n",
        "### cleaning validation set.\n",
        "lines_valid = Path(\"valid\").read_text().strip().splitlines()\n",
        "df_valid = pd.DataFrame([parse_line(line) for line in lines_valid])\n",
        "count=50\n",
        "df_valid= pd.concat([df_valid[0:0+count],df_valid[300:300+count]]) \n",
        "df_valid=df_valid.sample(frac=1,random_state=1234).reset_index(drop=True)\n",
        "df_valid=df_valid[['words','intent_label']]\n",
        "df_valid.replace({\"intent_label\":intent_map },inplace=True)\n",
        "df_valid.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_BiAez58BqF"
      },
      "source": [
        "## Defining the transformer model \n",
        "We will be using [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers) to train our model. They are a wrapper library based on the [Transformer](https://github.com/huggingface/transformers) library of Hugging face. It let's you solve many interesting NLP problems with a few lines of code. Initially, we define the model. Here we've used the 'Bert Model' for training our classifier, but you can also try it with other [pre-trained models](https://huggingface.co/transformers/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9vEyLgAqCio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7331af-e488-4c9e-8124-f0635f9bfce1"
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\r\n",
        "import logging\r\n",
        "\r\n",
        "logging.basicConfig(level=logging.INFO)\r\n",
        "transformers_logger = logging.getLogger(\"transformers\")\r\n",
        "transformers_logger.setLevel(logging.WARNING)\r\n",
        "\r\n",
        "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir=True, manual_seed=42)\r\n",
        "model = ClassificationModel(model_type='bert', model_name='bert-base-uncased', use_cuda=False, num_labels=2, args=model_args)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZG5kX9yrdoC"
      },
      "source": [
        "After it has been us talking for some time, it's your turn! Write a function \"get_labels\" that takes as input a prediction and returns the corresponding intent from the intent map that you saw above. This essentially means that if the input to the get_labels function is 0, it should return \"AddToPlaylist\" and if it is 1, it should return \"PlayMusic\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S87vregbgl5X"
      },
      "source": [
        "## TO DO: Write the get_labels function that converts the numerical predictions (0 or 1) to the actual labels (AddToPlaylist or PlayMusic)\n",
        "## You can either hard code the solution or – for a bit more challenging and in real-life more expandable implementation – try to use the mapping that is stored in the intent_map that we saw earlier!\n",
        "\n",
        "def get_labels(predictions):\n",
        "    labels = ...\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dknpLB6vtAn7"
      },
      "source": [
        "Good job! Now that we have that, we'll train our model. To ensure that training won't take too long, we set the training dataset size to be 0.1. There is a gap (indicated by the three dots) in the code for you to fill out though before we can continue :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue8hX0nCf_uU"
      },
      "source": [
        "train_size=0.1\n",
        "df_train_small= subset_datafarme(train_size)\n",
        "df_train_small.shape\n",
        "\n",
        "## Training the model\n",
        "## TO DO: Add the missing input parameter\n",
        "model.train_model(...)\n",
        "\n",
        "## Evaluate model\n",
        "result, model_outputs, wrong_predictions = model.eval_model(df_valid)\n",
        "\n",
        "## Print the loss\n",
        "\n",
        "print(result['eval_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93T40KMtkNR"
      },
      "source": [
        "Now we finally get to the fun part – using our model to predict the intent of a sentence! Again, we have left some gaps for you to fill:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF9Pu1FGtwQO"
      },
      "source": [
        "## These are the sentences we make predictions for like \"Play forties music on Pandora\" or \"Play the song domino by Luca Turilli\". \n",
        "## TO DO: Add two relevant prediction sentences like the ones above in string format to the prediction_sentence array (where the three dots are)\n",
        "\n",
        "prediction_sentences=[\"...\",\n",
        "                      \"...\"]\n",
        "\n",
        "## TO DO: Fill out the missing input parameter for the predict function and see whether you can actually get predictions for your\n",
        "## defined sentences!\n",
        "\n",
        "predictions, raw_outputs = model.predict(...)\n",
        "\n",
        "## With the following lines, we'll translate the predictions back to the intent using the get_labels function you designed earlier\n",
        "for i in range(len(prediction_sentences)):\n",
        "    print(prediction_sentences[i],\":\",get_labels(predictions[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aCL72tSuhtB"
      },
      "source": [
        "That's another step done and dusted :) Now it's time to see how our model is doing overall by examining the accuracy scores. First, we will define a function to obtain the accuracy of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJFazdKeWv3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def accuracy_model(model,df_train_small):\n",
        "    model.train_model(df_train_small)\n",
        "   #result, model_outputs, wrong_predictions = model.eval_model(df_valid)\n",
        "    predictions, raw_outputs = model.predict(df_valid['words'])\n",
        "    return  accuracy_score(df_valid['intent_label'],predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlXuZA8tvH3j"
      },
      "source": [
        "Next, we'll train the model with different training data sizes and save both the accuracy obtained (saved in accuracy_plot) and the size of the dataset (saved in train_shape) so that we can plot it later. Try it out with a couple of training sizes yourself – add four to five values between 0 and 1 to the training size array and see how your plot in the next cell changes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c3OsLeTeWx6"
      },
      "source": [
        "## TO DO: Add different training sizes between 0 and 1\n",
        "train_sizes=[..., ..., ..., ..., ...]\n",
        "\n",
        "train_shape=[]\n",
        "accuracy_plot=[]\n",
        "for train_size in train_sizes:    \n",
        "    df_train_small=subset_datafarme(train_size)    \n",
        "    acc=accuracy_model(model,df_train_small)\n",
        "    print(train_size,len(df_train_small),acc)\n",
        "    accuracy_plot.append(acc)\n",
        "    train_shape.append(len(df_train_small))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgI76r9vUgX"
      },
      "source": [
        "And lastly, we want to plot the training size against the accuracy obtained. You know the game, fill out the input parameters to plot the size of the dataset on the x-axis and the accuracy on the y-axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBHHLbe2ecZf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## TO DO: Add the missing input parameters\n",
        "plt.plot(..., ...)\n",
        "plt.xlabel(\"size of dataset\")\n",
        "plt.ylabel(\"accuracy obatined\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1ncITdEGj2"
      },
      "source": [
        "Good job, you're done with this part of the assignment!"
      ]
    }
  ]
}
