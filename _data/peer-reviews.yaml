-
  group: abaitch,jrodman
  project: ParkYeah
  suggestions: <ul><li>This seems like a very ambitious project, with a lot of interesting ideas to think about. I would recommend rethinking the use of workers in this scenario, not as a live output but as a hourly update service which may be more realistic given the time delay and processing buffer needed for data aggregation.</li><li>If the app showed specific spots that were currently open, instead of general data, it would be much more useful.</li><li>I would consider not using crowd workers who look at satellite images because satellite images do not refresh often. Instead, I would think about asking the users to leave a review/comment on whether the app helped them find parking.</li><li>Overlay the map of congestion with a map of roads where parking is allowed, if feasible. Otherwise, you may need to hack around this issue by hardcoding certain roads that cannot be parked on.</li><li>You might want to try working with the idea that users could submit when they are leaving a parking spot and then that would alert people looking for spots nearby.  It would be really nice because if you were in a really parking congested area and have to drive around the block, then you would get a notification of where and when one has opened up.</li><li>Maybe implementing a few Waze features, such as manual reporting (of parking congestion or relative parking space abundance) so that you aren't entirely relying on just GPS data? (Look up how Waze works if you're unfamiliar, because this could be a very helpful feature if implemented.) This would also get rid of the need of outsourcing to crowdworkers via satellite data.</li><li>I would focus more on how you would incentive people to participate.  How are you going to identify who is currently searching for parking?  Will they have to actively engage a mode to report that they are looking for parking?  If so, will people do that when they are busy driving?  Also, if there is any way to incorporate a larger crowd, it might be more effective.  I wonder if there will always be enough people looking for parking to enable the app to provide useful results.</li><li>You need to develop a system to actually indicate when a car arrives / leaves a spot. Also, incentivizing people to report such things is very difficult. Will you pay people? Will you gamify it?</li><li>To fix this you could try implementing it by doing something similar to waze, and having users show where they are and what the conditions are there. The problem would be trying to incentivize them, but maybe you could have a points system?</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 5<p>Fair&#58; 2<p>Poor&#58; 1<p>
  incentives: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 1<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It will be tricky to harness so much GPS data and display it on a heat map as shown in the concept video. There are inherent privacy issues with people who do not want to show their location to random strangers on the internet which ill have to be thought about. The biggest issue however is the delay between posting a HIT on Crowdflower and actually receiving enough results that can be aggregating. The main issue was trying to find parking spaces faster, but waiting for workers on the internet to sort through GPS data is not the best/fastest solution.</li><li>This is a great idea, but it'll be very difficult to pull off.  One big problem is getting people to actually open the app when looking for parking.  Another issue is that the heat map generated would be specific to where each person goes, and with a lot of data, the map wouldn't be very valuable.  Also, for all of this GPS data, there would need to be a lot of server space that we just don't have.  Lastly, the heat map, if complete, would show hot spots near Downtown and a cooling trend as the users get farther from the center.   I don't see how this would help people park, because it's already obvious that farther away there will be more parking.</li><li>The main problem i foresee with this project is the accuracy of the machine learning algorithm. As we saw with the gun classifier, the accuracy was very low. Users would not be pleased if they spent their time driving to where the classifier predicted there would be parking, but ended up wasting their time and gas.</li><li>I think the skill level needed by the workers to find open areas in the city to park may be above the skill of the Crowdflower workers. For example, there are several swaths of areas in Philadelphia that are technically available for parking based on satellite imagery, but it is illegal to park on these roads.</li><li>Is the idea that when a lot of people are looking for parking in an area, then that area is worse for parking?</li><li>The largest problem I foresee is the implementation of the machine learning algorithm to determine parking availability. I'm confused as to how exactly you envision this working off just GPS data—like if, once a user opts into Parking Mode, you mark any streets they don't park on as congested, and any streets they do park on as available? Will this be reliable? Will the user appropriately be reminded to opt in and out of Parking Mode?<p>I also don't think it's very feasible to implement a crowd-sourced satellite image parking congestion feature, because that would require having some sort of money to pay the crowdworkers with (unless you plan on charging for this app). <p>Lastly, I think recruiting the crowd for this app would be a huge undertaking, because you basically want the entire metropolis of Philadelphia plus commuters to download this app, otherwise it won't work. That's pretty intense!</li><li>Crosschecking the machine learning algorithm with satellite images does not really seem feasible to me.  Where would you get access to real-time images?  I know that the images available through Google Maps are long out-of-date.</li><li>Again, recruiting the crowd will be very difficult for this. Also, there are definitely other apps that exist like this.<p>Additionally, you refer to users' GPS as the location of parking spots. However, where is this GPS coming from? The car? The user's smartphone? If it's the former, I'm not sure how you'll get that data. If it's the latter, then you have to consider that people leave the car and take their phone with them. If it's a matter of self-reporting of when you park your car, then quality control is a big issue.</li><li>I think it would take a lot of data to map out people's parking habits in order to give a good estimate of the probability of parking spots in different locations.</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: abaitch,jrodman
  project: Campus Cribs
  suggestions: <ul><li>Clearly formulating strategies to attract/recruit workers and plan to aggregate worker responses will make the plan a lot better structured and will go a long way towards its feasibility.</li><li>The problem lies more in the idea than the specifics.  I'd scratch this idea.</li><li>No suggestions, but I do like how this idea can be implemented at colleges other than UPenn.</li><li>Some sort of verification from the landlord to confirm the listed rent amount.</li><li>To really be successful you would need to get quality information from established realtors first.</li><li>I actually think this is a pretty straightforward project that doesn't need much critique, because all things will work themselves out in the process of the site being created. I just think there needs to be a very definite timeline in order for this to be useful (at least for this upcoming housing cycle).</li><li>The video was only 57 seconds long, so there was not very much detail given.  They need to flush out their ideas more and clearly plan what would actually need to be done and how each stage of the project would be accomplished</li><li>Add a payment system so people can pay people from a house to pass the lease down to them.</li><li>I think it would be better if you had the crowd suggest other campus apartments for people, with the trade off being that your house gets a higher status if you recommend houses for other people.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 2<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 6<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 3<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 7<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 5<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It will be difficult to judge the accuracy of the worker responses. For example, you can't tell if a particular worker response is representative of both parities in a trade. Aggregating this information may prove to be a challenge due to the open-ended nature of the responses that workers are expected to provide.</li><li>I don't see many people using this service.  While it could be a great idea, spreading the word and populating the site with houses and apartments would be very difficult.</li><li>The main problem I foresee with this project is whether rising seniors would  really wish to trade their houses because by senior year they may have already found the house that suits their needs.</li><li>Not sure whether they have properly accounted for the incentive to lie about the listing. You could always post a lower price to gain interest before raising it upon actual communication.</li><li>I foresee trouble creating a platform where students can really communicate what they are looking for in an off-campus house.</li><li>I think the largest problem (which is actually a very big problem, and may heavily impact feasibility) will be getting the site up in time to be pertinent and useful; most leases require re-signing around November, so the site will have to be up ASAP in order for users to sign up, post profiles, and find buyers/sellers. Another problem will be actually recruiting users to the site, although I do think this is a useful idea, since a lot of people want to find entire houses to pass down and take up.</li><li>It does not really cover how they are going to recruit the crowd at the beginning.  It might be slow to be adopted without some sort of plan to bring in a first wave of users.</li><li>The main problem here is user adoption. The thicker the market is (i.e. the more houses posted on Campus Cribs), the more valuable it comes. So it can be difficult to start this up.<p>Also, this is more of a matchmaking service than a crowdsourcing project. I think it's a really good idea, but it isn't totally in line with what we've been doing in this class.</li><li>I don't really understand how this uses the crowd to match people with housing. It seems like more of an automated house matching system with little to no input from the crowd.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 0<p>
-
  group: abaitch,jrodman
  project: PennCrowd
  suggestions: <ul><li>Get some Crowdsourcing angles into the project!</li><li>Nope! I'm excited to see if this works out!</li><li>No suggestions, but if the university does not let you use the PennCard API, you may still want to consider doing this with geolocation.</li><li>This project is really tough and requires a critical mass of data gathering that seems to be unreasonable. if you do pursue this idea, begin by piloting a small subset of people and observing their movements around campus.</li><li>I think you need to figure out a way around the PennCard API problem, but I'm not sure how you would incentivize people correctly.</li><li>I don't think PennCard swipes will be the best way to determine study space crowdedness. For example, you can't be sure if people are swiping in to join a group study session (doesn't increase crowdedness) as opposed to claiming an entire GSR/Rodin booth by themselves. Furthermore, this won't help with determining the availability of high rise floor lounges (since everyone swipes in just to go home), or specifically WHERE the study locations are—sure, x people swipe into Van Pelt, but which floors are crowded?<p>Again, maybe implementing more Waze-like feedback mechanisms could be more helpful for generating better data.</li><li>I would investigate additional means of getting the data you need.</li><li>You would have to provide some service in exchange for students agreeing to share their location info 24/7. Perhaps you could gamify it. I could see it being something like a ubiquitous FourSquare, except you're always automatically checking in to wherever you go</li><li>To incentivize them you might use a system where if you let someone know that there's an open spot near you, then next time you're looking for a spot, Penn crowd will give you some options, and maybe the more spots you offer, the more options you get in return.</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 1<p>
  incentives: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 2<p>Poor&#58; 2<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 2<p>Poor&#58; 2<p>
  excitement: Excellent&#58; 0<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 3<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 3<p>
  relevance: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 1<p>
  problems: <ul><li>While this a great idea there is one very large problem - there was no mention of any kind of crowdsourcing at all. Assuming you'd like the workers to identify which buildings are most busy...you could do that automatically (and freely) with a few lines of code. There really isn't any aspect of the project, directly related to its aim, that involves crowdsourcing.<p>Another issue would be getting access to PennCard data as I'm sure it would be private. There would have to be talks with security departments to get access to aggregate numbers rather than individual logins for privy concerns.</li><li>Just that getting the data from the university might be difficult, but otherwise it seems like a great idea!</li><li>The main problem I foresee with this project is whether the university will allow you to use the PennCard API. Other than that, this seems like a good idea.</li><li>Not very sure as to how many Penn students use Foursquare enough to reach a critical mass of people necessary for this measurement. PennCard swipe in time is also an inconsistent indicator even if you get that piece of data because I could have swiped in to use the printer or swiped in and camped out to do an essay.</li><li>I can't imagine Penn would ever release that information.  I don't think there's a PennCard API and a quick google search seemed to reinforce this.  I'm not sure that there is an incentive to tell the truth if everyone is looking for a quiet place, then wouldn't people in quiet places post that it's really crowded, so it seems like nobody else should go there?</li><li>PennCard API access... red flag!! Aside from that, I think it's a great idea. The automacy of it is definitely the app's greatest draw.</li><li>People swipe into locations, but they never swipe out.  It might be difficult to gain an accurate picture of how crowded different locations are just by looking at penncard swipes.  Additionally, places like huntsman only require swipes late at night and on the weekends.</li><li>Again, recruiting the crowd is practically impossible here. I doubt that Penn would allow you to access students' PennCard swipe info.<p>Even if you were to access that data, it is very far from complete. Swiping into Huntsman isn't even always the case. You have no idea when people leave Huntsman. You don't know where in the building people are. Even if I did know that exactly 274 people are in Huntsman now, what does that information even mean to me? What I really care about is whether or not there are open seats in the silent study area.</li><li>I can see people using the app to check where to go to study, but I can also see them putting in false information when they want to keep a quiet study space empty. So the problem is incentivizing them.</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 3<p>Poor&#58; 0<p>
-
  group: egenene,rkitain
  project: Ant Sirs
  suggestions: <ul><li>Instead of asking for someone to do your homework, why not make the crowd provide a background study guide which would enable the student to understand and answer not only his own question but similar ones, too?<p>The voting system for data aggregation is solid. Maybe include an incentive for those answering questions to receive karma or credit or something so that while students are waiting for voters, they can be sure that the person who answers them is trustworthy. <p>Don't change the logo; the logo is great. <p>Find a better definition of project success. Your current definition is unclear. Make it quantitative and easily measurable.</li><li>Perhaps, have the user always pay the one crowd member/group that provides him/her with the best answer, and all others don't get anything, similar to models we've seen in class.</li><li>Possibly shift this idea to something that people would more likely use without getting in trouble -- possibly a question and answer site like StackOverFlow, where people can get help with certain problems without being blatantly cheating.</li><li>In terms of project itself, try to come up an interactive interface to collect results and ensure the quality during the aggregation. It will also be good to consider the problem mentioned above.</li><li>You could make it more learning oriented, as opposed to just letting the student pay for the straight up answer, so that it's less unethical. Also narrowing down the possible topics for homework would make your idea more feasible I think.</li><li>I would limit it to people who have taken the class the homework is for. Obviously the monetary incentive will have to be higher because you need to pay college educated students for their time instead of random people on crowdflower who are trying to make as much money as possible in the shortest amount of time.</li><li>Maybe looking over homework instead of doing it would be easier to find people willing?</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 4<p>Fair&#58; 2<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 3<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>A number of competitors exist with systems already in place - Yahoo Answers (though admittedly that one is terrible), Stack Exchange, Chegg, and a hundred other sites. <p>It encourages plagiarism by asking for answers (Ant Sirs) instead of background on a topic, which would be an interesting concept.</li><li>How are you going to prevent blatant cheating and foster learning?<p>How are you going to ensure that answers really are correct, especially for problems that might not be obvious, and would therefore go by rounds of crowds just saying that the previous round's edits made sense.<p>Will people get paid at the end, if/when the user is satisfied with the answer?</li><li>Other than ethical issues, there may b problems with the quality of the solution. This type of idea may be helpful for easy problems or small portions of certain problems.</li><li>The project is coming out of good intention of making students succeed, but might have potential problems with student conduct.</li><li>With homework, there is typically deadlines, so working out how you're going to reach the desired quality in what might be a short amount of time (depending on the students own procrastination) could be a challenge. Also, you didn't mention how you know the crowd is done creating an answer.</li><li>The main problem is that turkers are essentially unskilled workers, so you can only assume they know how to do very basic homework. I doubt you can post a CIS 160 problem on crowdflower and eventually get the right answers from people. Even if many people attempt it, you need someone with the right expertise to answer the question.</li><li>I think most crowdworkers willing to do this aren't educated enough to do this so you'd have to go about finding people (students) willing to do this where you can be a tutor for a lot more money<p>Also there's no clear way identified to figure out if the answers you're given are right or not - and it sounds really challenging to do this. I like the idea though &#58;)</li><li>It will be hard to find the right niche in the existent Q&A field. Isn't Stack Exchange very similar? Also, some questions may not be answerable by most members of the crowd.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: egenene,rkitain
  project: Crow de Mail
  suggestions: <ul><li>You mention a worker voting system - how will you iterate on poor initial response? Will you use multiple starting points for iterative improvement? If so, how will you aggregate these? - Consider these ideas. <p>Ask the user to rate not only speed but quality. <p>Implement an inbox that can handle email chains, crowdsource them, and return to the user with the same contact handle.</li><li>Perhaps given a draft email, this could provide slight changes that could soften or harshen the tone, be more persuasive, etc. This seems like a much more viable option than actually drafting an email from nothing.</li><li>Possibly find ways to make the requester experiencer as easy as possible. Since this project seems to be most likely used for shorter emails, filling out what should be on the email may be more time consuming. Provide workers with very high quality work with even more benefits (gives them more incentive to do more work in the future).</li><li>You might want to define more on how much information a user need to provide to let the crowd finish the rest. For the provided information, what aspects need to be covered and how to better instruct the crowd to finish the rest.</li><li>If it's for english as a second language have the user submit a draft of the email in their language then have it be translated by the crowd. That's not really any different from something like duolingo though...<p>If it's for people not bothered to write their own emails, I would suggest the user submitting a draft of his email then have turkers iteratively improve it and vote on the different versions.</li><li>How about creating a script that will help you reply to meeting requests etc? Want someone to reply to an email for you? forward that email to an address that you've set up and have them reply (maybe you'd have to give them access to your email) but it would look like you had a secretary while really you just have a turker.<p>hi thanks for responding, nick (cc'ed) will help you schedule a time for us to meet. nick is the crowdsourcer name.</li><li>I really like the idea of crowd workers reviewing the user's email responses before sending it out. This way, the crowd could help teach the person how to respond in emails.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It's very similar to the iterative inbox reader mentioned in class, which is already in production. <p>It isn't clear how the context of the email will be provided. Will you be creating an interface to permit users to forward an email chain, or drag & drop an HTML file? If the user has to create a job for each email, it's not worth the effort, but creating an interface to do all this is challenging and perhaps unfeasible in the course of this semester. <p>Success is based on rapidity but not quality of responses. There are no quantitative metric targets that would make success easily measurable. <p>This product would serve non-native english speakers, but the crowd recruited by mturk or crowdflower would likely be non-native english speakers themselves, or at least a significant portion would be.</li><li>How useful is this project, where the time could be spent actually writing the email? i.e. if someone can't write an email, how would they be able to describe aptly what needs to go in it and how it needs to be written?<p>What about the sensitivity of certain emails?</li><li>I feel that this is similar to the idea behind Soylent, but with a more specific target. Privacy issues may come up and deter people from participating; however, this could be reasonable with short not-too-private emails. It may be difficult to find quality workers, and the results may not carry the same connotation that customer had envisioned.</li><li>This is somewhat similar to some projects we have seen in the class but a very different implementation. One concern is definitely the privacy issues and people tend to have tones and emotions when writing, which is hard to be reflected by the crowds.</li><li>The aggregating the turkers' input part was vague, so I'm not sure how they plan to do this, but how they choose to do this will effect the quality or efficiency of the idea.</li><li>I don't really understand if this is supposed to be for people who speak english as a second language or just people who aren't bothered to write their emails? For the first case, I can see the use since google translate is so bad. But for the second case, if you're gonna spend the time submitting all the details of this email and what you want in it, why wouldn't you just write it yourself. It would take significantly less time and it would be free</li><li>Potentially privacy issues could come up and also making sure that qualities are well-written. Many crowdworkers are not that educated so having them write better emails than you could seems very unlikely - particularly given you have to already write a summary of the emails you want written so it's unclear that it will really save time.</li><li>It will take some work to decide on what work the crowd is capable of responding to. Many emails require personal information to respond to. It may be easier to have crowd workers evaluate the user's draft responses than to make responses themselves.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: egenene,rkitain
  project: FowlCmdy
  suggestions: <ul><li>Choose a more objective measure of success, like an increase in followers or a certain ratio of favorites or retweets to each tweet. <p>Find a way to limit geographical area of responses if you are targeting, for instance, Americans. At the very least, responders should be evaluating humor in their native language.</li><li>Maybe break down the crowd to rate and identify by types of comedy, that way it's easier to quantify how funny each are. This way users can also just look up funny tweets by category and ranking. You should still have an overall funny rank, but this might be harder to choose for.<p>I like your idea of the test questions, but maybe add more trials on already answered tweets to compare answers.</li><li>Maybe choosing a topic other than comedy would be a bit easier. Comedy can be a bit tricky because a tweet may be trying to be funny or just plain funny because of its content. Also think about more concerete methods of aggregation and quality control.</li><li>The concept of filtering out good content on the Internet is great. But you might consider implement it on another platform and find out a standard way of defining good content as well as a good incentive for people to do so.</li><li>Maybe have workers try to categorize their humor or even the joke as dark, punny, etc. I read a book on the science of laughter, and there has been research attempts to identify what makes something funny or what categories exists that might be helpful and are really interesting.</li><li>In order to fix the problem I described above, you would need to use the crowd a lot of times on the same tweets to ensure that they're actually funny and its not just one person finding it funny.</li><li>Instead of doing it based on humor, maybe pick another heuristic - like sports, or current events, or people. Or generally sarcasm, you could potentially pick a vertical of humor and attack that.</li><li>Try to automate as much as possible. Maybe you could look for comments which people retweet, or which people respond to with a lol or haha. There's definiitely a considerable amount of computation which could be moved from the human back to the computer. This would also minimize costs and increase quality.</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 0<p>OK&#58; 5<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Comedy is extremely cultural and relies on context. Crowdflower workers may not understand the joke and filter out quality material, or allow bad material to pass through. <p> <p>Funny content may already be identified by the number of followers on a self-proclaimed funny page.</li><li>How accurate of a funny filter can you get, with funny being so subjective?<p>Will the time, effort and money that goes into this beat a specific twitter handle who is already curating material that any user finds matches their flavor of comedy?<p>It's very easy to feign a response just to get money on crowdflower for this type of task, it seems like it will be hard to do quality control against.</li><li>Opinions on what is funny may vary from person to person. There are also many different types of humor. Also, filtering for funny tweets may be a problem. How exactly would it be filtered? Using keywords? Using a crowdsourcing method? There are milllions of tweets a day, and finding funny ones may be quite difficult.</li><li>The crowds are Twitter users which are very sparse and hard to reach to massive size. The incentive is not that tangible/concrete, thus hard to implement.</li><li>Definitely quality control. They said the workers will vote yes it is funny, no it is not funny, which would be really easy to just randomly click through or create a bot to do. There's also the success measure. They said they would look at the top results and hopefully see what is funny, but it's a very vague evaluation and different people have different senses of humors or have or lack the background knowledge of why something is funny.</li><li>I really like this project. The main problem I forsee is what people deem as funny is not gonna be the same across the population...</li><li>1. I am skeptical a machine learning algorithm will be able to really tell you which tweets are funny. Twitter is so now-focused that jokes about different current event topics are constantly changing, and the core of humor is so different - that it seems unlikely that there will be stable correlations between words found in humor and real life. (sarcasm is hard enough to detect in person).<p>2. someone without a strong English background or pop culture understanding might have trouble understanding the humor and might not find it funny.</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: elijahv,levjam,petrol
  project: Eventor
  suggestions: <ul><li>Their may be a wide range of events suggested by workers which can span across cultural and religious barriers - this is also a great learning opportunity for the user, and could be incorporated in a quantifiable manner.</li><li>Maybe make it a local thing so that they can run errands and really do all the hard parts of organizing a party.</li><li>Perhaps implement a payment scheme whereby requestors pay the business, who then pass this money along workers who's work is chosen</li><li>I think it would be better if the user had to fill out some sort of form about the event that they want planned, and based on the answers, certain tasks are sent out to workers to be completed.</li><li>1. Try to cater this to longer term events</li><li>Make the template so users can control what suggestions they need.</li><li>I like the idea, but the funding aspect may need to be clarified a bit.</li><li>I like the blind date example. This is a situation where creativity is always welcomed. Maybe focus on situations like this one?</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>How do you check for quality of work? There are a range of acceptable responses, which makes quality control tricky to handle.</li><li>I think that it will be hard to give someone else sufficient access to the event details, guest lists, venues, and payment methods to truly be able to plan a party for me.  Most of what's hard about planning a party is running errands.</li><li>Figuring out who is going to pay seems like a slight issue, since requestors might lie and pretend to reject all solutions.</li><li>I think getting workers to work together is difficult. It seems as if the way this would work is that the user posts the details of an event and the workers can decide if they want to do parts of it or the whole thing which doesn't make much sense to me. It would make more sense to divide all of the small annoying tasks up and distribute them to the workers. Moreover, if the workers whose work gets accepted are the only ones who get paid, it's unlikely that many people will put that much time into it only for their work to get rejected and receive no compensation.</li><li>1. It will take time to aggregate everything and the events might be occurring too soon for everything to be planned by a crowd</li><li>Need to make a template of suggestions - everything from food to furniture to guest lists - where will this template come from? How does this replace an event planner in real life?</li><li>Will event planners work for such a small sum? How much will users pay for the service? All of the other workers reviewing the event also seems expensive.</li><li>People may not want to disclose details about certain events to random other people. They also may not have time to wait for responses from the crowd when and event is imminent.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: elijahv,levjam,petrol
  project: Food Flip
  suggestions: <ul><li>Try expanding the functionality of the project to including more than just recipes - common foods to avoid, best tasting foods for certain restrictions, good retriction-suitable restaurant chains etc. Might be pretty cool to get operational!</li><li>Try riding the gluten free fad!</li><li>Perhaps the site can generate a list of recipes for you given a set of dietary restrictions.</li><li>I think this is actually as good as it can be. Very similar to StackOverflow in theory, and although they both have their flaws, StackOverflow had done amazingly.</li><li>1. Develop an incentive scheme for initial users. <p>2. The site might be able to be expanded to highlight the top contributions</li><li>Expand it to not only restrictions - but what foods can anybody make given a set of ingredients.</li><li>I think it's a very good idea.  Maybe restrict the number of questions users can ask until they contribute more, so that way people will be incentivized to answer other users' questions.</li><li>You might benefit from focusing on smaller categories of food restrictions, like being gluten free. Here, life and death is not always a concern, and people can really benefit from a community-like forum.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 1<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It might be difficult to attract a crowd of workers all facing food restriction issues.</li><li>I think that it will be hard to gain the traction needed, and I would be a little worried about accidentally making people sick with bad substation suggestions.</li><li>I am worried that its targets a niche market that is too small to be sustainable and that any existing crowdsoucred question/answer site can probably accomplish the same goal.</li><li>I think it'll be pretty hard to recruit the crowd necessary for this, and there's no way of checking whether or not the members of the crowd have any experience cooking with food restrictions.</li><li>1. It will be difficult to get a system coded and up and running quickly<p>2. The user base is also limited so participating incentives initially will need to be thought through.</li><li>Hopefully the recomedations are from people who know about the diseases (celiac, gluten free, vegetarian etc) so may want to focus on quality control</li><li>People may not contribute as there is no direct incentive for them, e.g. they are not getting paid.</li><li>People with these types of allergies, where life and death may be a legitimate concern, may be hesitant to trust random people from around the internet. They already use books and websites that provide professional (sometimes doctor-backed) suggestions, so why risk it?</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: elijahv,levjam,petrol
  project: Room Tetris
  suggestions: <ul><li>Coming up with a different incentive system could help the project a lot.  Perhaps the site could pay workers rather than the requestors themselves (possibly using advertising revenue).  This would make requestors more likely to truthfully report their choice.  Or rather, perhaps the business can be paid upfront by requestors, and the business would then pass this money along to workers who come up with chosen solutions.</li><li>I think it could help to have the workers vote on the best set up done in parallel and then improve on it in series to make it the optimal organization.</li><li>1. How about only paying for the winning answers</li><li>Could have smaller steps, like does this furniture match the set - or how have users list 'themes' they want for the room.</li><li>I think this is a very good idea.  Have you taken into account vertical dimensions as well, to create they best 3D layout of the room?</li><li>This could be pivoted to focus on interior design. Many times new homeowners have no idea how to go about decorating their homes. This could include wall colors, textures, and furniture colors and styles. Let the crowd build a visually appealing and comfortable room.</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Would hard to generate an interface to drag/drop items which can handle a range of items and item dimensions.</li><li>I forsee having trouble developing a drag and drop interface like the ones pictured without some serious coding/design knowledge.  I mean I'm sure it can be done, but I would suggest scaling that back and starting with just text suggestions from pictures (multiple needed for quality suggestions).</li><li>One problem I foresee is if the requestor likes a given setup and decides to use it, but claims then he/she rejects all solutions in oder to avoid paying the workers.</li><li>I think it'll be hard to determine the amount of money to pay workers since it seems like a pretty time intensive HIT&#58; too much money, and the project becomes too expensive, not enough money and no one will want to spend the time doing it.</li><li>1. Recruiting the crowd might be difficult depending on how much people are willing to pay<p>2. If there are a lot of responses to a question that are bad, it may cost the user money for no reason</li><li>3D imaging, physciality of space - is there a substitute to actually being in the room? Breaking these barriers</li><li>Might be a lot of information for users to input in order to have the crowd arrange their room.  Measuring every piece of furniture may be a lot for some people, but I really like this idea.  Would people with design skills take the time to plan these rooms for a small sum of money?</li><li>I find it a little bit of a stretch to think that someone would take the time to measure their room, measure each piece of furniture, put all of that information into the system and then wait however long it would be to get suggested results back. During that time they can easily move stuff around their room until they find a fitting configuration.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: mosnyder,tifflu
  project: Venn
  suggestions: <ul><li>Maybe having suggestions what two people can do when they meet up based on their interests will encourage every match to meet up.</li><li>Integrate with social media to prove that people know each other, and to increase accessibility for a larger crowd and better results. While your interface may be cool, embedding it within a familiar site would be easier for users. <p>Handle geographical distance in some way. For example&#58; if the distance between two users is greater than the threshold distance either of them has set, they are never compared as potential friends. <p>Do you want to introduce people if they already share a network? How different should their social circles be? How will you determine whether or not two people have already been introduced? Determining these things how you see fit for the vision of your project will improve it.</li><li>I think when people directly talk to each other they can know best if they are compatible.Perhaps add a chat window for ppl to talk, instead of having<p>data/likes and dislikes etc.</li><li>This sounds like a really cool project, I think for generating a proper crowd, you should pick the type of crowd you want to aim for and consider the pros and cons of each. I think the two different sides to consider are do you want an already heavily connected crowd (something like hinge, which I think is well aligned with your incentive structure and is probably more likely to lead to your measurement of success, i.e., making new friends, since I'm probably more likely to be friends with someone I have mutual friends with) or a large general crowd (more like tinder, aggregated by location so physical meetups are easier, but while this would probably be easier to get a bigger crowd in terms of size, it might not be as good a crowd, since people will not have the incentive of knowing others in the network and also might not be as likely to actually be friends, meet up, etc.).</li><li>I think it's a really interesting idea. I would just explain more about the user adoption strategy and how you will get and keep users participating. For example, maybe the only way to get matched is if you build up enough points from judging others.</li><li>I would consider weighting the opinion of someone who is friends with the two people being matched (maybe pulled from Facebook) higher than the opinion of someone who has no relationship with either person.</li><li>Look into how facebook suggests friends, and look at algorithms for matchmaking and even dating sites</li><li>- Thin down the questionnaire portion of the idea to a manageable size<p>- Design an easy way to evaluate the success of finding friendships, for closeness between users, choose a metric early on to quantitatively assess progression</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 1<p>
  incentives: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There might not be enough users joining the platform or users won't be proactive about establishing these new relationships.</li><li>Many apps already exist like this. For a Penn-specific example, check out Down to Chill, which operates like Tindr, but is for platonic hanging out. The competitors you mentioned, as well as broader competitors like meetup.com and other forums, could draw away users. <p>For these reasons, it would be difficult to recruit an unpaid crowd, and having a paid crowd seems incompatible with the core ideas of the project (friendship).</li><li>Getting people to join.Also, how well can someone judge if two others will be friends just be written interests?<p>Many thins are left out in such a study.I believe this will be difficult too execute.</li><li>I think the biggest issue will be generating a good crowd. You'll need a crowd that is (1) big enough, (2) active on the site, and (3) that is interconnected since a big part of the incentive scheme is that you know other users for whom you are matchmaking.</li><li>Recruiting the crowd is the biggest problem here. For one, just getting people to download and use the app will be difficult. You need a critical mass in order to even get it functioning on a low scale. Then, your plan to gamify the matchmaking is interesting, but I'm not sure if it provides enough of an incentive to get people to really think hard about two people and whether or not they would be friends. Even if they do, it is hard to tell if any arbitrary person's judgement would be at all accurate to real life.</li><li>Most of the problems that come to mind are those faced by other matchmaking-esque services such as false profile information, spam, etc.</li><li>Very complicated, how will one determine what questions are key to matching friends?</li><li>- The idea relies heavily on having a base network effect, which is difficult without recruiting a seed group of users.<p>- The friendship algorithm is something that would need to take into account user answers, profile traits, connectivity, etc. -- hard to evaluate on correctness and difficult to choose a method to implement<p>- The data collected from user Q/A will be very noisy (e.g. what happens if users don't answer truthfully, if users purposefully troll or answer randomly), have to find a way around the noise which won't be easy without a large dataset to work with</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: mosnyder,tifflu
  project: Radicle
  suggestions: <ul><li>Have really great publicity and I think this will be a really interesting and awesome project!</li><li>Use pre-built interfaces and forum hosting, focus on survey creation and data visualization. <p>Introduce some type of data quality control, like dummy questions to ensure users are paying attention, in order to achieve the most organized results. Some surveys that I have taken through the Wharton Behavioral Lab impose a minimum time limit on each page of questions to ensure better data when users take their time. <p>Place some mechanism of anonymity of users to ensure greater honesty, or come up with a different incentive that doesn't broadcast anyone's status as a victim or perpetrator of sexual assault.</li><li>Be sure to cover security issues.There could be personal information in the <p>forms.<p>Make all forms anonymous.</li><li>Insert dummy questions with definite right/wrong answers to ensure users aren't randomly answering. Find an alternative method of incentivizing users instead of or in addition to monetary compensation to counteract some of the issues described above.</li><li>In order for a student to post a question, she must first answer a certain number of surveys. The more surveys someone does, the more points they get, and points allow people to promote their survey.</li><li>At a high level, this project seems similar to Quora, so maybe look into how they effectively attract high quality answers from the crowd.</li><li>Market this as the go to resource for the consensus of people - (ie who won the presidential debate according to positive and negative tweets/fb posts) this can be the new go to for crowd results.</li><li>- Have a way to choose important, exciting questions, or else all of the top crowdsourced questions will be revolving around the same topics all the time.</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 6<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 4<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There might not be enough individuals from Penn willing to answer the questions on the survey.</li><li>Feasibility&#58; rating of perfectly feasible based on testing or forum phase, not rollout, which may come later than the end of the semester. <p>There are no mentions of data quality control in the survey, only the relative ease. While there are no financial incentives for cheating the system here, it's important to take these measures prior to data aggregation. <p>What is a winning answer to a survey on campus sexual assault? Motivation of students through glory is deeply concerning, considering the private nature of disclosures of these kind.</li><li>Is there anyway to make this profitable?How will you incentivise people to actually fill out forms?Often penn students are too busy.<p>Information leak issues and privacy concerns are another thing to watch out for.</li><li>I think the incentive structure is tricky. I agree that you should make the questions simple and easy to answer (yes/no, strongly disagree/strongly agree), and the only real way to incentivize people for this project is with money. While simple questions will keep costs low because you won't have to pay as much per question and people are more likely to actually participate, you run the risk of people just clicking through and randomly answering, and since these are opinion based, it is hard to tell when this is happening. However, making the questions more complex (free response, for example) means you'll have to pay workers more, the data will be harder to aggregate, and it'll be harder to get users to answer a high volume of questions.</li><li>Think about how difficult it is to get Penn students to participate in any survey. Why would anyone want to actively seek out surveys and take them just for fun? There are professional polling companies that work very hard to recruit people to take their surveys and pay them fairly good money to do so. Will your service be much different?<p>Also, the popular questions with the most upvotes may not be the most valuable questions. Important questions such as ones about sexual assault may not be seen as interesting by the crowd.</li><li>The main problem I see is ensuring the quality/relevance of responses. For example, if the question posed is a poll targeted at female Penn students, you will need to ensure that the respondents from the crowd fit that demographic. If people are being paid to respond, they are incentivized to respond and ignore demographic requirements.</li><li>How do you ensure the sample size is accurate/with the appropriate and knowlegdable audience?</li><li>- Paying on crowdflower to get user-generated questions and answers isn't likely to give back any real dormant questions, since it becomes a task<p>- How many questions are interesting to a large number of people at the same time? Most groups of people tend to be interested in subtopics, and there are few things that inspire many people who ask and answer questions regardless of audience<p>- Seems like Quora or Reddit</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 6<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: mosnyder,tifflu
  project: Mooch
  suggestions: <ul><li>It should have a good messaging system so that communication between the two people can be smooth.</li><li>Add some information on how you will market this platform to people who are willing to give, as they are putting themselves at risk. Givers are somewhat more necessary than moochers in the early stages, because they will be harder to recruit. <p>Collecting the venmo account information of a moocher who does not return things could allow administrators to cover the losses of a giver, ensuring that the platform does not become toxic and lose its crowd. <p>Provide broad categories of rentals of varying lengths - perhaps even of textbooks.</li><li>Answer questions on quality control better.<p>Ask for fixed deposits from people who wish to become takers for goods.</li><li>I think to fix both the problems described above (and I think you mentioned this in class) is to start by having the crowd be purely college students (require users to sign up with penn emails perhaps?). This way there is more accountability and trust between lenders and moochers. People are less likely to scam each other since they are not complete strangers. Deliveries will also be much easier within a school bubble because of physical proximity. You will also be able to get a larger crowd because people will be less wary of giving a large amount of personal information for pickups (phone number, address, etc.), giving you more potential users, and college students are also the most likely demographic to use/need this the most.</li><li>To help build trust, maybe this could start off as a co-op-like system, where people have to be vetted in order to get into the system, just like the food-sharing company that Ellie presented a few weeks back. Also like that company, maybe you could also introduce delivery people to deliver and return items.</li><li>Look to eBay for how to effectively act as a middleman and ensure quality when exchanging money for a physical item.</li><li>Work on incentivizing bigger mooches for those with bigger points, (a TV/speakers for a party for level 100 users versus level 1 users just a lamp)</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 1<p>
  incentives: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 3<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There might be unreliable lenders or borrowers but that should be weeded out by having a review system and profile confirmation.</li><li>Merely evicting moochers who do not return an item seems inadequate if a college student loses property without any recourse. <p>Typically the items listed are things borrowed between friends or social circles for free. How will you recruit a crowd who may not require this service? If it's by allowing them the chance to give, and make money, more incentive may be necessary.</li><li>Quality of products, chances of people returning items.<p>Going by ratings has many flaws.Firstly, first time users may defect as they have no ratings.<p>IF there is damage, on an expensive item, who will pay? <p>What if someone steals an expensive item on their time using the app?<p>Who will pay the giver?</li><li>The only issue will be bad participants&#58; lenders who say they have something they don't, false advertising, delivering late or not at all, etc. and moochers who don't return their items, return them in bad condition, return them late, etc. Generating a crowd large enough and with a wide enough variety of resources may also be difficult.</li><li>Quality control is a huge problem here. People can post pictures that they find online of items. How will you weed those out?<p>Also, generally building trust is a big issue. Lots of people are very skeptical of meeting up with people on Craigslist, even though all money exchanged is done in the flesh. You talk about building a sense of trust amongst the community, but this will certainly be difficult.<p>One other problem&#58; there is nothing mentioned of delivery. Will people be responsible for delivering things to one another?</li><li>As you mentioned, I think the main problem will be with un-returned items and ensuring payment.</li><li>How is this different then free and for sale (aside from the borrowing dimension). Sounds more like an honest forum than crowdsourcing.</li><li>- Dealing with money is difficult from a product perspective, lots of edge cases to consider (how to punish toxic users)</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: agadi,chenylei,gitlesb
  project: WikiTrend
  suggestions: <ul><li>I'd love for the idea to be more thought out.  It seems to have potential!</li><li>Think through the questions above and reformulate the plan. The base idea is definitely an interesting one, but it doesn't seem to develop to something feasible or useful past the initial idea.</li><li>Needs more clarity and thought.</li><li>Maybe scale back and just focus on the key words instead of trying to get the summaries as well.</li><li>So my understanding is the articles you're going to use the number of clicks on a wikipedia article to get your data set. There is a possibility that there will be two+ articles that arise in your data set that although are primarily about different things, but a related news headline (e.g. Putin and Russia or Chinese Communist Party and Hong Kong). So allowing the crowd, especially if the same turkers do your task multiple times, to tell you if they've seen a similar news topic before would be a good since it get rid of costly repetition.</li><li>Maybe it would be cool to recommend trending articles that the user might be interested in?</li><li>I would analyze social media data rather than wikipedia click data. It's much more current and can paint a cleaner picture of the volume of people that care about or are talking about certain topics.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 1<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 3<p>Poor&#58; 2<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 3<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I'm not totally sure I understand the idea.  It has to do with machine learning and using the crowd to teach an algorithm how to notice news trends using Wikipedia.  One main issue would be finding people who would give you their browsing data.  Also, getting the data from Wikipedia could be an issue, as it may not be public or tracked as closely as necessary.</li><li>The idea as a whole does not seem to be very well thought out. I feel as though it may make more sense or be clearer in your head, but I did not feel as though the proposal explained it as thoroughly as possible. It also seems as though there are several details that have not been discussed or thoroughly planned out. These details must be discussed or addressed if this project is to be realistic. Some questions I still have are&#58;<p>- What exactly is the information you hope to extract from the wikipedia pages?<p>- How will crowd workers contribute?<p>- How is this useful?<p>- What aspects are crowd sourced, and what role will your algorithm play?</li><li>This idea isn't well thought out enough; many parts of the planning are missing. It's also unclear what are the real trending topics -- people's ideas of news vary quickly, and I'm unsure that major trends are not revealed. The answer and video don't provide a clear sense of what the problem being addressed is, or how it's being solved.</li><li>Difficult execution with a disorganized procedure and goal</li><li>I'm not entirely sure how the algorithm works, but the implementation will be the most difficult part I think. I'm also not clear how you are aggregating the crowds data, but it could lead to coherence issues.</li><li>I don't see how this is much different from obtaining data on the most popular wiki pages. Especially if personalization isn't involved.</li><li>Wikipedia clicks may or may not have anything to do with current news trends. There are a lot of factors that can increase or decrease the amount of times users click on certain features in a page, and they may very well have nothing in common with current events.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 3<p>Poor&#58; 1<p>
-
  group: agadi,chenylei,gitlesb
  project: First Impressions and PSG Elections
  suggestions: <ul><li>Instead of a simple left or right for choosing who is more competent, it'd be a good idea to have some sort of metric for better analysis.</li><li>Overall, I think this idea is very interesting and I would like to see how it turns out. I also think it is more feasible and more thought out than the other two proposals. <p>In case you realize that your data set is too small, I am sure it would be fairly easy to get the candidates and results from past years, which would greatly increase the number of comparisons available, thus increasing the odds of the results being reliable.</li><li>Chris's suggestion in class could be interesting -- applying the idea to Congressional elections instead, where there are no/less prior biases.</li><li>Maybe a game like interface to get more participants and giving them their accuracy or closeness to the majority's choice.</li><li>I like this idea a lot. I think you might benefit from focusing on crowd workers from a similar geographic region and, if possible, similar socioeconomic backgrounds to the average Penn student.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 7<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>None!  Seems like a great idea!</li><li>The only problem I foresee is that workers may realize the pattern where the actual questions can be randomly answered, and the quality control ones should be answered with caution. Although this is a possibility, I don't think it'll pose a large problem in terms of the data collection because each worker can only answer a limited number of questions, and so they will not necessarily skew the results too significantly.</li><li>Not having enough money on CrowdFlower to get the amount of data they'd like.</li><li>Quality control</li><li>Quality control, but I thought their solution was quite smart.</li><li>This is really cool and interesting. As you mentioned, the cost factor could be an issue.</li><li>People may have cultural biases over factors such as race and gender. This is especially important because crowd workers may come from all over the world and may not emulate the judgements of the average Penn Student.</li></ul><p>
  quality: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: agadi,chenylei,gitlesb
  project: PennRepairs
  suggestions: <ul><li>None!  Sounds great!</li><li>I would suggest forgetting about the classifier aspect of the project, and focusing instead on building the best interface that would encourage Penn students to participate and post pictures of various things that need repairs.</li><li>Find out of there's a potential partnership with Penn maintenance, and perhaps scale down the scope (highrises, Quad, etc.) -- better target range / easier to repair.</li><li>Using one layers of crowd sourcing only. If I'm willing to take a picture of something that needs repair, I can surely add a sentence describing it (maybe even choose from a drop down list).</li><li>Maybe you can focus on a specific aspect of campus and spread awareness about a specific problem through other means. That way when people see an instance of the problem (say, broken lights mean a more dangerous walk home for students at night) they are more likely to stop and report it.</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 3<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>One big problem would be getting the university to start using this application!  Also, it might be tough for crowd workers to decide which tasks should be completed first, but for the most part I think they would be pretty accurate.</li><li>The main problem I see would be incentivizing the Penn students to submit the pictures of things that need to be repaired. Unless there is some fun aspect to using the app, or some other incentive, then I do not believe that students would use the app. <p>My biggest issue with this project is that I am not sure if the purpose is to build the best classifier (to determine what kind of repair is needed, the priority of the repair, or how serious the damage is), or to build the infrastructure that enables students to effectively communicate with the people responsible for repairs.</li><li>There are a huge number of problems on campus -- some of the problems may have been processed and already dealt with by the time this project has assembled the pictures and gotten the crowd to vote on it. Furthermore, unless Penn maintenance decides to join this effort, this won't do much good.</li><li>You didn't mention how they will find out when a fix has been completed, so they may amass jobs that have been completed lingering on their site for a long time. Also actually getting response to the things that need fixing, but that's really out of your control.</li><li>It's a really cool idea, I can't see many major problems here!</li><li>People are often in a rush and don't have the time or incentive to stop, take out their phone, open an app, snap a picture and write a description of the problem. How would you deal with this?</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: jenhu,zhouwe
  project: Quakr
  suggestions: <ul><li>A test could be implemented for Turkers to ensure that they would be a good fit for the matching task. The crowdsourcing could change so that instead of Turkers, friends within a community could match up people they know. This could increase willingness of users because of the personal relationship and increased trust with those matching them. A test to see how well Turkers perform could be to use current couples (could even be celebrity couples) and see if Turkers suggest a match at a high enough rate.</li><li>Doing matching based on something very specific might be more interesting. Maybe something based on people's stream of consciousness, or their response to some kind of prompt.</li><li>A strict system that cuts out poor crowd reccomendations may improve quality quickly.</li><li>Maybe also implement a computer algorithm and see if the matches from that are the same as the matches from workers and only then suggest it to the couple, for more accuracy?</li><li>1. Incorporate friends' opinions on the person in addition to their self-description and profiles<p>2. Hide more sensitive information to further anonymize the users</li><li>Don't black out the pictures when Turkers are evaluating profiles. May sound shallow, but that's a big part of first impressions.<p>Maybe have the Turkers evaluate the weight, or likelihood of compatibility based on a pair of profile attributes. They would independently assess compatibility based on two people's political views, hobbies, area of study, etc. More work but could be more controlled.</li><li>Ask the users a few personality questions that you think are relevant</li><li>It's a cool idea. I think adding a machine learning element would be better. Get crowdsourced data to match subsets of profiles / specific fields and then use a machine learning model for matchings there on out.</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 6<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>A big one is quality control. Turkers are not always reliable, especially in such cases where the answer is not a clear objective one. Since Turkers are from all over the world, and are also mostly male, the opinions could be flavored by different customs or experiences. It could also be difficult to get people to sign up for this service because they may not trust the advice of people who they don't know. Also, would the people looking for a match have to pay for the crowdflower workers? The description does not make it clear whether it is going to be implemented on a mobile or web system.</li><li>What information are you providing for these Turks to make their decisions? A lot about this process is unclear.</li><li>Participation</li><li>What qualifies the users in making their recommendations? This also seems very subjective.</li><li>Since mechanical turkers are international, I think cultural differences could lead to matches that in the US would not be considered ideal.</li><li>1. Different people have different opinions about what a good match is - some people think two people who are extremely similar will get along very well, while others think differences can act as complements.<p>2. Privacy concern - even though pictures and names are hidden, a lot of other personal information will be given to anonymous Turkers and users might not be okay with it<p>3. Sometimes people are very bad at describing themselves and it might not even be accurate</li><li>Sensitivity about predictions. If someone is slated as a good match for you, is it definitely reciprocated for them? How do people connect after getting matched up?</li><li>Make sure the turkers have enough useful info to make a match, especially if you only use text.</li><li>So privacy of the people getting matched is an obvious issue. Ensuring good matches while maintaining anonymity will be an obvious challenge</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 0<p>
-
  group: jenhu,zhouwe
  project: Food Trafficking
  suggestions: <ul><li>The food truck owners could also be offered the option to update users on the wait time of their food truck. This might be a problem, though, for food trucks that are busy because updating users on the long wait time could drive them away, which is obviously unfavorable for the food truck owners. A stronger incentive system would also help increase the user base, such as earning points for food. The quality control also needs to be addressed more thoroughly to ensure users don't game the system. Testing the project could be done with a group of crowdflower workers constrained to a geographic area.</li><li>Including efficiency as well. Some places may have a lot of people but move much faster than others.</li><li>Maybe notifications would be useful to users reminding them to use the app near meal times.</li><li>Find a way to ensure that the time that a person gives a crowdedness rating is also the time that they are there, maybe through using pictures?</li><li>1. Perhaps developing it as an add-on to Yelp's check-in feature might be helpful - many people check in when they go to a restaurant anyway so rating the traffic wouldn't be too much an extra step.<p>2. It might be more helpful for the restaurants themselves to rate the traffic than the users - since they are more incentivized to let people know their current state and they will be there all the time to keep updating and make sure no information submitted is outdated</li><li>Bring food truck workers into your crowd. They can report if they're line is especially short or long at any time. Especially if they have no line, they'll want to broadcast for more customers to come.</li><li>If you want to get early adopters you will probably need to provide incentives. You could also pair up with a food truck to get them to advertise your product.</li><li>Adding a feature about viewing trend data would be good. Additionally integrating the app with LevelUp or any other virtual payment method would encourage people to go on it more / provide an unbiased data stream.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 5<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>The incentive for people to post the status of different restaurants isn't clearly explained. The group suggests an elite status will be awarded for frequent users but it is not clear what that status will be or how it will differentiate such users from normal users. Also, if restaurants aren't crowded, it seems likely that people would not bother posting. It is also possible that users lie about the crowded status to reduce the crowd and help themselves get food faster. The quality control method (limiting it to Penn students) doesn't seem effective; the email or facebook checking method doesn't seem like it will be effective in identifying users who may game the system. If testing is done with a group of friends, the system could be tested but the usage amount would probably be skewed since friends are more likely to use it as a favor to the developers.<p>If the user base is geographically expanded, a system to identify and inform users of only food trucks and restaurants close enough would have to be developed.</li><li>How do you know that you'll be able to get the data when you want it? How will you incentivize initial users to get onto the app? For many of the places Penn students go to around campus, the general busy hours are already known.</li><li>Not getting enough people for good updates, and checking the accuracy.</li><li>The incentives seem to be weak. What reward does the elite status mentioned give users?</li><li>I think that writing a platform for this project could be cumbersome and time intensive. Also, when getting food, people often just want to eat it and might forget to give the real time crowdedness ranking until later, which could lead to inaccuracy. This is different from yelp since yelp's ratings are not real time dependent.</li><li>1. How long will these traffic ratings last? One hour might cause a major difference in the traffic in a restaurant so outdated ratings should be cleared out efficiently<p>2. How to incentivize the crowd? Yelp's model works because people naturally like to take pictures of food and recommend restaurants and be considered restaurant pro, but rating the traffic is only helping others but not showing any quality or the users themselves, so why would they take the extra step to rate it online (especially for food trucks where people are usually in a hurry)?</li><li>Active participation may be difficult. People are lazy and already have a lot of apps!</li><li>It's a cool idea but you will need a large user base to make it useful and getting the first group of users could be difficult.</li><li>While the incentive system has been adequately described, it doesn't account for the fact that only hungry students will actually be active on the app. Passive participation is going to be foreseeably lower. Also such data will be very time sensitive.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: jenhu,zhouwe
  project: PennImpact
  suggestions: <ul><li>Having some way to communicate to administration or creating a petition from the top requests would be useful. An option for users to work together to write a petition for the top requests and gain signatures through the site is also relevant. A clean organization system that categorizes the different types of requests would be useful in the implementation of the idea.</li><li>Sample proposals/templates</li><li>Because this is only a student body then maybe a few admins could screen all the suggestions made by students to make sure irrelevant/inappropriate material is kept off the site.</li><li>Definitely implement a rating system to show currently available petitions. There would probably be a lot of petitions that are relevant only to a small number of people/are for a joke which you wouldn't want to have coming up high on search results.</li><li>Cooperating with the UA or other student government branches can help legitimize the cause and get more attention from the Penn administration</li><li>Add a social component so users can see what others reported -- maybe aggregate this into a map</li><li>Talk with penn's social impact groups as you build the product to make sure you have the right feature set. This could also be useful for recruiting users.</li><li>If you could collaborate with the UA / Student Govt or EDAB etc to have them use the platform to get opinions from the penn community about a potential new reform, it would be like a referendum system hosted online helping each organization better understand the needs of their electorate.</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 7<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 3<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>The idea is a good one but it could be hard to test by the end of the class. The incentive to participate as described makes sense but some more details on how the Penn student population will be educated on the existence of the platform could be used. Also, is the platform limited to undergraduates or will it include graduate students and professors? Will the questions or requests be categorized? There also needs to be a way to bring administrative attention to the problems. The quality control of rating and upvoting sounds like a good and necessary idea. The main problem is finding a representative crowd to test the platform out.  Crowdflower workers don't make sense in this case.</li><li>There already have been many petitions started by Penn students that have gathered a lot of signatures -- what will make this platform more likely to be taken seriously? Also, how will you sort between serious/not serious requests?</li><li>Participation and getting Penn administrations actually involved with it too.</li><li>The quality control is a bit fuzzy. For the site to be taken seriously there needs to be high quality suggestions from students.</li><li>Again, feasibility of writing a web platform that has a lot of traffic (students always have something they want to change...) could be a problem.</li><li>1. It will be hard to actually get the attention from the Penn administration and get them to cooperate<p>2. Penn students are generally very busy and it will take some extra steps to get them involved</li><li>The quality of the crowd input. I'm still unsure how you'll guarantee people are actually posting real problems that need to be solved.</li><li>Getting the site up and running could be difficult if you don't have experience. Also getting users will be a bigger issue than QA.</li><li>The project implements a specific version of Change.org for Penn. It doesn't implement a new idea or a more specific idea. Change.org would work equally well.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: avenkata,jston
  project: CrowdCheck
  suggestions: <ul><li>It may be helpful to limit the scope of the project to specific types of agreements. The pre-test may also contain more examples than just legal terms.</li><li>Include a basic glossary of legal terms for workers.</li><li>Before moving forward with the summarization or labeling of a database of ToS's, I think it may be beneficial to experiment with a couple at first to see what HIT structure gives the best results. The group could manually label or summarize a couple of ToS's, and then post various HITs that are phrased differently, and that rely on different workers working in series or in parallel, until an efficient and effective method is found.</li><li>Try employing the crowd to check the work of the other workers <p>Come up with an aggregation scheme</li><li>You could provide Turkers with a few key terms/lines that are known to be related to aggressive ToS clauses.</li><li>Maybe have the option of sending terms and conditions which aren't already pre-processed to the app so it can return the result.</li><li>Using a more specialized crowd to label positive and negative terms. The crowd could be used to tokenize each ToS and then a Machine learning algorithm could provide results regarding it's sentiment as positive or negative.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I think that scraping agreement information from various sites would be a problem. I'm not sure how readily accessible they are, and there are millions of them out there; some are actually used while others are not. Also, what is considered a good Terms of Service? People may have different views.</li><li>Some workers may have trouble with potential legal nuances. Portions of the privacy policy may refer to other portions of the privacy policy; this could make splitting tasks up by paragraph difficult.</li><li>I think the initial problem the group will have is building a database consisting of the ToS of several different companies. Furthermore, the software built in this project would need to detect when the terms change so that the new terms can be reevaluated. Failure to do so would result in out of date summaries of the wrong ToS. <p>As mentioned in the proposal, it may be difficult for the group to come up with an adequate and informative enough set of labels. I also think it would be difficult to phrase the HITs in a way that guarantees that the crowd workers produce the optimal results.<p>If these issues are overcome, I believe this would be a very useful plugin to have.</li><li>1. Difficult to aggregate results<p>2. Difficult to judge accuracy <p>3. Need to label consistently</li><li>Some Terms of Service may need more context than the paragraph you show a crowd worker so their opinion may not be correct. Furthermore ToS may require a decent amount of working knowledge in a domain that could be unfamiliar to turkers.</li><li>Similar tools might already exist. It's really a great tool to encourage people to read license agreements!</li><li>ToS agreements tend to be tedious to read and easy to misinterpret due to the legalese. The crowd may not have the adequate background required for such a semantic analysis.</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: avenkata,jston
  project: DealFlow
  suggestions: <ul><li>This project could be a bit less specialized. This seems to have very targeted consumers and a specific type of crowd to aggregate data from. People may not be likely to want to take part in the project. Maybe something more small-scale like the Penn community?</li><li>The project seems overly ambitious. Consider limiting it to just one sector of the VC industry for the project.</li><li>I would suggest that the group undertake this project in a different setting (meaning outside of this class). I think the scope of the project is too big, and the timeline would need to be much longer.</li><li>Try to improve the incentive/crowd recruitment scheme</li><li>Have turkers search online for examples of companies in each sector. If you do that then they won't need any domain knowledge.</li><li>Maybe have built in themes and focus on crowd sourcing the projects that match the themes?</li><li>Move from generating a platform run by VCs to a platform run by the generic crowd who can talk about a new app or any new firm they've come in contact with. VCs can then use this data to contact the specific firm. In fact, you could charge the VCs for sharing data.</li></ul><p>
  feasiblity: Excellent&#58; 1<p>Good&#58; 0<p>OK&#58; 5<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 5<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 5<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>This task needed for the crowd seems a bit specialized; they would need to know about a range of smaller, private companies, which a lay person would probably not know about. In terms of quality control, many random companies that people suggest may exist but not be doing very well and would not be worth it to fund.</li><li>Getting the desired crowd could be very difficult. Many VC companies have access to proprietary information; individuals may be unwilling to just give up their list of contacts and post them on the platform. The fee structure could be difficult to set up as well.</li><li>The main problem I foresee with the project is getting entrepreneurs, VC analysts or firms, and other industry people to use the product. I do not believe that it is possible for the group to build the product, and manage to convince enough desired people to participate for there to be any monitoring of how successful the project is in time. If the desired crowd is unavailable, then simulating this crowd using friends or crowd workers would greatly diminishes its value. For this reason, I do not think this project would be feasible in the given timeframe, but I do think it would be interesting to see how it would work out in a different setting.</li><li>1. Recruiting the crowd will be tough, it's people with a specific skillset<p>2. For those with the skillset, it will also be difficult to motivate them financially as their jobs will be well paying already <p>3. Not sure VCs would trust it</li><li>Actually getting business professionals to engage with your project before the final could be challenging.</li><li>The main problem I foresee is how to get the crowd (business people who can provide propriety deal flow).</li><li>Gettings VC specialists to crowdsource ideas they get paid millions to project would be especially hard. 1. It would make a lot of super experiences people redundant. 2. It would mean that VC executives openly reveal what company they're looking to pursue etc. Not a great business strategy.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: avenkata,jston
  project: MarketChatter
  suggestions: <ul><li>Possibly flesh out a robust way to scrape off data and clearly define from where these articles/comments would be coming from.</li><li>Consider comparing company sentiment with changes in stock value and other metrics.</li><li>One main aspect of the project that the group failed to mention is how the actual analysis would take place. It is not as simple as having one negative tweet balance out a positive one, or one negative article balance out a positive one. When asking crowd workers for their view on the sentiment of the article or tweet, it would be helpful to have it be on a scale of 1-10, rather than a positive/negative/neutral. Furthermore, a tweet from a user that has millions of followers should be weighted more heavily than one with just 5-10 followers. The same holds true for the articles--an article posted on the NYTimes should be weighted more heavily than one in a tabloid.</li><li>1. Try to incorporate more crowdsourcing into the project, right now you're not really using or recruiting any particular crowd per say <p>2. Figure out an appropriate way to analyze large amounts of data<p>3. Figure out a way to judge accuracy</li><li>Since this has been implemented before you should study those who do it effectively.</li><li>Move to a sentiment analysis based machine learning algorithm to observe and process data. Change the application to work for more specific and less instantaneous market events &#58; eg&#58; IPOs, Stock Splits etc.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 1<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Like our homework assignment with gun violence, the hardest past of this project may be the aggregation of data about companies from social media. Many posts on social media and web articles may contain facts about companies without any blatant sentiments toward it -- it may not be obvious if the sentiments are positive or negative. Also, many posts may just contain the company's name without any further information. As the team has mentioned, controlling for the most recent data may be a problem.</li><li>Developing a machine learning algorithm to determine sentiment may be difficult (crowdflower may be an easier way). Company sentiment will vary by time so the time of the tweet will need to be taken into account. Some companies may not have enough tweets made about them to determine an accurate company perception.</li><li>One important issue that the group addressed in their pitch is the fact that all data they scrape for has to be current in order for the sentiment analysis to be effective. It may prove to be difficult to ensure that the data being analyzed is not out of date.<p>If the group is able to extract data in such a way that the data is current, and the group is then able to come up with a clever way for conducting the analysis, then I think this project is very feasible and interesting. I have provided some suggestions of things to think of when the group works on the analysis part below.</li><li>1. If MT is not used, this isn't really crowdsourcing but more or less just data scraping<p>2. There might be too much data for a human crowd to analyze<p>3. Difficult to judge accuracy of sentiment analysis -- nothing to really compare the output of your program against<p>4. Scraping at such a large scale while using NLP tools may be difficult to implement</li><li>One potential problem could be quality assurance from the crowdworkers (you only mention QA for twitter users)</li><li>Nothing really, the project sounds great! <p>A small issue would be finding a crowd of journalists and users to actually use and contribute to the app.</li><li>Markets are far to rapidly affected by news for such a project to be relevant. Crowd may not have financial experience required to accurately describe news articles or posts</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: drajan,scwu,selld
  project: Identifying Geographical Biases in Gender Roles
  suggestions: <ul><li>The team could limit the number of people who can fill out the survey from each country. They could stagger the release of the job to a convenient time in each time zone. <p>Regarding translation, you could crowdsource that for a slightly more reliable data set, perhaps even using google translate to generate a translation, then suggest it as a possibility if the weight of the feature exceeds a certain threshold.</li><li>Make your process more closed-ended. Instead of asking open questions, show pictures or blocks of texts and ask users to pick which one they identify with more. If location is hard to measure or gain a significant size for, use a different identifier like race or socioeconomic status.</li><li>For ensuring data quality, I like the idea you have about including dummy questions to ensure people are actually answering and not just randomly clicking. It also might help to alternate between simple questions so people will actually answer them and free response questions to ensure people are actually paying attention (although these might be harder to aggregate results for). It sounds like a really interesting research project that definitely would have wider implications, but you may want to go deeper into how you'll measure the success of this project, and what the data you gather shows, can be used for, etc.</li><li>Figure out how to evaluate any bias that results from using crowdflower workers.</li><li>Making a classifier to only predict gender seems a little simple. I would suggest adding a second factor, like predicting the age of the respondent or something along those lines.</li><li>If it is the case that you are considering both of the focuses mentioned above, I would recommend choosing one or the other. My choice would be to focus on geography as it relates to gender roles, because I think this is a more novel and interesting topic that I would be really excited to hear more about.</li><li>I would suggest finding statistics on the demographics of CrowdFlower workers and clarifying the project's scope. Since this project is a bit more research oriented, I think it's important to specify exactly who you guys are trying to get data on and which types of societies you guys are examining. For example, maybe you could focus on targeting different geographical locations across the United States.</li><li>Find a low-bias method of interpreting results, maybe do a test run of questions to see the variance within the responses and judge whether those questions are accurate.</li></ul><p>
  feasiblity: Excellent&#58; 8<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  problems: <ul><li>How will the team guarantee a diversity of geographical areas in their crowdflower responses? <p>Diversity in geographical location means diversity in languages, which means recurrent phrases will not hold the same weight for the classifier, unless you rely on a translator. This will introduce an additional element of complication that should be considered. <p>Aggregation of responses needs to be considered. Which 1000 responses will be selected as training data, if you are expecting different results based on region?</li><li>It is unclear as to how you will be able to achieve a statistically significant survey size from the various locations around the world that you are trying to survey. Also, the demographics of Crowdflower are unclear to me, so I don't even know if you can reach countries like India or China through Crowdflower. Finally, the machine learning/HCI aspect of this project is unclear and/or limited.</li><li>Since you're only incentivizing people with money, and you're keeping questions simple (which is good to ensure more workers and more questions answered), you run the risk of people arbitrarily answering to get paid. I'm also a little unclear on what the final goal is and the measurement of success.</li><li>The crowd on crowdflower probably comes from a limited set of backgrounds, which means that it will likely hold some sort of bias one way or another on different issues.</li><li>I like the project a lot, but it's gonna be pretty hard to build an accurate classifier if you are going to survey people from different geographical regions. If you can get enough good data from the turkers it will be completely fine though.</li><li>One problem I saw from the video was an unclear scope of the project. I was not sure whether the primary focus of the project is on the relationship b/w geographic location & gender roles or if the focus is on the relationship between interests, future goals, etc & gender.</li><li>I feel that it may be difficult getting data from enough meaningful geographic locations. The workers on CrowdFlower may primarily only come from a few different countries, so the data received might represent as many cultures as the team would like.</li><li>- Self reported statistics tend to be biased<p>- The Crowdflower audience isn't controlled for region/gender/social class/etc., likely to have a lot of noise in the aggregate result.<p>- The process of aggregating said results will have a large influence on the final result of the project, and it's difficult to decide on the correct way to interpret the answers. (observer bias)</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: drajan,scwu,selld
  project: CrowdLove
  suggestions: <ul><li>Clearly define a threshold for what makes a match public post-worthy. Instead of removing bad matches, why not prevent them from being publicized? <p>At one point, it is mentioned that users will participate for free because it is fun, and then on the last slide, payment through crowdflower is mentioned. They can coexist, but this should be clarified. <p>Assessment of success should include some measure of overall experience, not simply the quality of the matches. <p>Geography needs to be accounted for - users should be able to specify some radius. How will you accommodate this?</li><li>Use Twilio to text the two people if they get a match. Maybe do a slight pre-population of interests by the users of the service so that you at least have a bare minimum of information even if Crowdflower workers can't get any more.</li><li>I think to combat the issue of people's social media profiles being the sole basis for judging people, you could instead use a crowd of mutual friends (more like hinge), perhaps having networks of only students at Penn (or any college). That way, you may be able to set up your mutual friends, acquaintances, etc., and you also have a better understanding of who the person is (combating the problems with self-evaluation) going off of more than just a facebook profile. I think this would also incentivize matchmakers since people would probably be more interested and invested in creating matches for people they know/know of.</li><li>I would work on the two points that I outlines above.<p>For the first one, I would ditch social media profiles and just give people a more standarized list of info. Some pictures, what the person likes, and things along those lines. It will just make it easier and more standardized for every user.<p>For the second, instead of asking them to pick from multiple choice questions, I would just ask them the question straight up and have them type out the answer. You can then compare what people say about the person and pick the most prominent descriptions (again using the crowd)</li><li>I would suggest creating a person's profile based on a combination of user-submitted and crowd-submitted attributes, rather than crowd alone. This way, you could potentially filter out bad responses from the crowd.</li><li>For the privacy setting problem, the only solution that I can think of is to request that users make their information public. However, this might discourage people from using the application and would lead to additional problems that would need to be addressed.</li><li>- Use a hybrid system where the crowd responses influence an ML algorithm on users' profiles to determine whether users match.<p>- Using the above could also lower the set of possible evaluations that workers would have to do (e.g. a quick automated filtering pass before pushing out surveys)</li></ul><p>
  feasiblity: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 0<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Privacy settings of users could bar the crowd from viewing profiles unless you extract the data from each platform and re-present it in your own format, which would be a bit of an undertaking to be completed this semester. <p>How will you prevent abuse or harassment of users who sign up? Their information is publicly available and the internet is full of nasty people.</li><li>This is a pretty solid idea, and the video definitely was well made and presented. One concern that I have is that not everyone is an avid social media user -- what would the Crowdflower workers default to if there is not enough information to generate a list of interests and hobbies? Also some people have privacy settings that make it difficult to view pictures if you're not a direct friend. Furthermore, even if you do get a match, how are you going to introduce them?</li><li>Although it makes a lot of sense that people have different perceptions of themselves than their friends do, I think social media profiles also often give off different personas than who the person truly is. Therefore, making matches just on this may not be the most effective. It's also a good idea not to incentivize matchmakers with money since they would be likely to just go through randomly answering, but getting a large enough crowd of people willing choose matches for fun might be difficult.</li><li>There would likely be a fair amount of contributors who would not take it seriously.</li><li>I see two main problems, the first having full access to people's social media and their pictures. Of course if you get someone to sign up for a service it will give you access to whatever data they agree on, but it's not feasible in this case. What will most likely is happen is that you'll have people's limited social media profiles. <p>The second is making the multiple choice questions that the turkers have to answer to give personality traits about the other person. I'm assuming one of you has to make this multiple choice list, so at that point why wouldn't you just pick the right answer yourself. Seems like a bit of a waste of time. I do like the final matching process though</li><li>I foresee two problems with this project. First, from a privacy standpoint, I think people may be uncomfortable with the idea of a group of unrelated strangers evaluating them. On typical dating sites, I think there is a comfort in knowing that you're seeing the profiles of the people who are looking at you. The idea that a separate group of strangers is looking at your profile could be unnerving for some.<p>The second potential problem I see is crowdworkers not giving earnest responses and attempting instead to work as quickly as possible. Because crowdworker responses are subjective, it will be difficult to institute quality control mechanisms.</li><li>There is a discrepancy between what the video says and what the Project Pitch says. The video specifically states that the match making process will not happen on a crowdsourcing website and that contributors will be contributing for free. If this is the case, I feel that there needs to be additional incentive to motivate contributors.<p>Another potential problem is that information on people's social network websites may not be available to the public due to privacy settings.<p>privacy to access<p>privacy for user</li><li>- Unlikely that a smallish crowdsourced opinion on matchmaking would be very accurate<p>- Asking people to judge others based solely on a social profile is difficult<p>- The risk of being incorrect is steep compared to how easily a worker could skew the results</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: drajan,scwu,selld
  project: RapGenii
  suggestions: <ul><li>If they could offer some kind of tangible incentive, like recording user-generated lyrics, uploading them to youtube, and crediting the author contributors, then the glory motivation may work in the earlier stages of recruiting a user base. If they can't provide more than a byline on an aggregated poem (the methods of aggregation need to be more clearly described), they should offer a financial incentive.<p>There should be a clearer benchmark of success, like generating x number of rhymes on y beats.</li><li>Create some prompts to help people get started on particular topics. Pre-populate the community with some set templates and original songs before releasing it to the public so they can go somewhere to see how to use the service.</li><li>Start out by offering monetary/some other form of compensation to get an initial user base. You already have some methods in place to help with continuity and overall product, but maybe adding a third step where someone reviews the entire product, or larger chunks to make sure it is cohesive.</li><li>You need to find a specific community of people who would be willing to do this. If you tap into the right crowd, I am sure rap enthusiasts will do this for free, or for recognition points.</li><li>Perhaps an additional incentive could be provided to get users to participate and to provide quality lyrics. Maybe a user with consistently highly rated lyrics could win a studio session to record their lyrics!</li><li>I would recommend offering a more practical reward for contributions. The project pitch mentions that the project is a business idea that uses crowdsourcing. Maybe, some of the revenue generated could be shared with the contributors. For example, if the application makes money by displaying advertisements, maybe some of that money could be distributed back to top contributors.</li><li>Find a use for rap god points</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 1<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It's a little bit of Twitch plays musician. If they can recruit a user base, it could be successful as an exercise in crowdsourcing (if not as a profitable business, at least by the end of the semester). If they can't - which is entirely plausible given their motivation scheme is glory, which requires a large user base to work - then they will be writing their own raps. <p>Providing the lyrics of other artists as inspiration may qualify as plagiarism, particularly with a lazy crowd, who may be inspired to copy and paste. <p>The assessment of the success of the project is subjective, and the alternative offered (the up- and down-voted scores) is relative.</li><li>I am having a difficult time in understanding why anyone would actually want to use this service. Rap god points isn't an adequate reward for the rather substantial process of creating new lyrics for songs. Another problem is that I don't see who would start a song and why they would choose to do so. Adding on to a song is reasonable if the payoff becomes legitimately valuable, but the incentive to start a new song is unclear at best.</li><li>Starting out, it may be hard to incentivize users to contribute. As a new network, points in a small crowd are not as important as in an already established forum. In aggregating the lines, you may also have an issue with flow/continuity since lines are coming from different users.</li><li>It might be harder-than-expected to form cohesive songs from a large number of contributors.</li><li>I think you would have to create your own crowdsourcing platform instead of using mturk or crowdflower because this is very different from the tasks people are usually asked to do on these platforms. It is doable, but given that the only incentive for the crowd is monetary, I don't see how someone would be bothered to listen to the beat then come up with lyrics.</li><li>The largest issue I could foresee with this project is participation. Normally, trivial and humorous responses would be a worry, but humor seems to be a goal of the project, so that is good. I can foresee it being a difficult hump to get over before people will voluntarily contribute.</li><li>I feel that the team needs a better plan to incentivize a crowd for this project. Although they do plan on giving rap god points to contributors, I'm not sure how much people will actually value obtaining the points. Especially since rap god points don't really have much practical use, I could easily see most people not being very interested in them.</li><li>- As mentioned, if there's a lack of users the project isn't likely to turn out hilarious raps.<p>- Users could troll and add/upvote some unsavory lines</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: holtc,jlaskin,mlabarre
  project: Lineless - Why Wait?
  suggestions: <ul><li>A better way to solve this problem would be asking a few people (or the shopkeepers themselves) to record the times during which their line is not as long. Not sure if there is a crowdsourcing application though.</li><li>This might work better if you tried to partner with restaurants, but I'm not really sure how that incentive system would work out.  Maybe if you started it at restaurants that pride themselves on speed and efficiency.</li><li>I don't know if you were already planning on doing this, but it would be awesome if you could check different lines by distance or type of food. However, I think the biggest improvement would be to have a more tangible incentive system. For example you could give people rewards like discounts at a restaurant for participating after 100 contributed updates.</li><li>Providing some sort of entertainment so that people will like using it, like Thank you for your participation. Good for you! This other place has this many more people, you're almost there!</li><li>The importance of an incentive for the crowd to participate should not be underplayed.  Perhaps the foodtrucks can offer discounts as an incentive for the crowd to provide information.  Additionally, maybe the crowd can simply check-in to a food-truck while they wait which sends GPS data to the app.</li><li>Instead of asking the people waiting in line, I would crowdsource it to the business owners to say if there is a long line or not. The incentives this way are more aligned, because the owners want to attract people to their business. Also, shakeshack does this in nyc and it seems to work pretty well, so it's probably a safer bet than finding a crowd of engaged users. I am sure the foodtruck owners have smartphones and would be willing to promote their business with one simple click</li><li>I would suggest thinking of a way to provide a more practical reward for contributors. I think this is necessary especially since this project requires a frequent stream of contributor input. I also think it's important to come up with a concrete plan for how you guys will actually evaluate the project. I suppose that you could get some friends to use the application, but I feel that it would not be very meaningful for this type of project.</li><li>Include food truck owners in your crowd. They could report short lines to encourage more customers to come their way (could be dangerous, too, though since they're biased by cash flow)</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 6<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I'm not sure how you will reach the critical mass needed to make this app really useful. Even having one person in line with the app downloaded and open is a large standard to strive for. There's currently no incentive to use the app, and the gamification is unclear.</li><li>I think the hardest problem here is getting people to use it.  There is also a clear incentive to spam the site as being really busy when you want to go there.  That way nobody would show up because they think it's really busy (even though you're the only one there).</li><li>I think it would be difficult to incentivize the crowd to participate. Gamification doesn't seem like enough to get people to log whether a line is long or not, but I can definitely see people using it to determine which restaurant they visit.</li><li>Quality Control, hard to estimate accuracy if there isn't enough participation</li><li>Obviously it would be tough to get people to adopt using using this app in a time window as small as this semester.</li><li>Foodtrucks may need an incentive to participate if they are to provide discounts for those who check-in while waiting in line.  (Why would a popular foodtruck want to scare away some potential customers if they know it will be busy?)</li><li>It's a very cool project. The main problem I see is that it's not gonna be too feasible to recruit a large enough crowd for this to work properly in this short amount of time. Also, I don't think gamification is going to incentivize people to respond because they do not have much to gain...</li><li>The team doesn't seem to have a concrete plan for incentivizing contributors yet. Gamification was mentioned, but I think it would be difficult to motivate people just by giving them points, etc.<p>Furthermore, this project requires a frequent stream of data, and I think it'll be unlikely that they will get enough contributors to actually execute the project.</li><li>Participation. People are lazy and already have a lot of apps.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: holtc,jlaskin,mlabarre
  project: The Music Database
  suggestions: <ul><li>Maybe some kind of music collaboration database? Basically an annotation of people who mash up two different artists' music (so it is unofficial music). One example of this is the Majestic Casual channel on Youtube.</li><li>If there were a way to discover music that would be cool.  Also, maybe the site could aggregate links to music videos.  You might have more success attacking this from the artists' POV.</li><li>Consider using Wikipedia's model of using crowdsourced editing.   Perhaps a music rating system similar to Rotten Tomatoes could be implemented.</li><li>I would make it more of a webscraping thing from websites that already exist. Then you ask the crowd to fill in the missing information that you can't get from scraping already existing sources.</li><li>I would suggest providing more incentive for contributors. I also think it's necessary to include some method of quality control. This is especially the case since anyone contribute to the database.</li><li>Right now, I think the idea is too close to wikipedia. If I google any music artist and go to their wikipedia page, I can see all of the albums, eps, mixtapes they've come out with. I DO like the idea of having a database devoted solely to music, completely with RATINGS. Maybe start the database by reaping data from wikipedia or elsewhere. Then, the crowd can continue to add to it.<p>Consider incentivizing participation with a social component. I think goodreads does a great job of this&#58; http://www.goodreads.com/</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 5<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 1<p>OK&#58; 6<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Not sure how this is very different from Rapgenius. Because of that, the users who may come to your service are probably on Rapgenius instead. It's also unclear as to how effective Crowdflower workers would be when assessing musicians and artists in the US.</li><li>I'm not sure how you will drive adoption with music fans.  I think Wikipedia already does this for the most part.  I guess it doesn't have ratings, but otherwise they already do this =/  That being said, I would use this site to look up information if it existed, but I'm not so sure I would sign up for an account to contribute.</li><li>Again, the incentive is the problem. I don't know if people will willingly spend time filling out information about artists, unless they're big fans</li><li>Participation and incentives</li><li>This is a really cool idea, but the timeline and scope are not within the realm of possibilities for this class. I think it would be difficult to build this website and teach people how to edit it in a short amount of time.</li><li>I am a little unsure why this is necessary when site like Spotify exist.  IMDb is useful because there is no such Spotify for movies.</li><li>The main problem is websites of this nature already exist. Platforms like beatport essentially already do this for you already. Also, it would take an absurd amount of time to aggregate data on all types of artists and their music. Also, music is subjective so it would be hard to implement universal ratings for music...</li><li>I did not see much about how quality would be controlled for this project. For example, how do you guys plan to prevent/address when people input incorrect information into the database?</li><li>Lack of participation could render the database useless. It's a really daunting task to information about all artists, and you'll need a lot of input before the site is useful as a resource.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 5<p>Poor&#58; 0<p>
-
  group: holtc,jlaskin,mlabarre
  project: Crowdsourcing for Cash Flows
  suggestions: <ul><li>Adding context to negative articles will be key to providing the whole picture for retail investors. Help them pinpoint the exact reason for the stock moving one way or the other.</li><li>I really liked your idea of narrowing your focus to tech stocks, but you might also want to ask something specific about the tweets like whether they think the sentiment is likely to last for a long time or not, and to what degree the tweet is emotional.</li><li>Have an interface in which the articles or tweets are already loaded so that the users don't have to go through the links one by one.</li><li>You may want to consider what you can do with the data output from the classifier - how can it be used to generate the revenue needed to sustain a viable business?</li><li>Instead of cash flow I would focus on stock prices. I would send turkers the latest data available from twitter and know sources like the WSJ and the FT. Since computers still struggle with natural language processing it would be a great way to get the sentiment of the article/tweet and then you would make the stock trade based on these sentiments (whether to buy or short). Think of what quant hedgefunds do with trading with the info on the internet, but adding the human component to ensure the right sentiment is accounted for since language can often be ambiguous.</li><li>I would suggest formulating a better plan for how to meaningfully use the data once it is obtained. I think this is necessary in order for the project to proceed.</li><li>Focus on one sector only during the testing phase. Since you're suggesting an end-to-end service of article analysis then predictions for these stocks, I think you'll find it easier to focus on only tech, food services, fashion, etc. I'm sure different sector will exhibit different stock behavior as a consequence of changing sentiments.</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 4<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I think this is a pretty cool idea that can have some real world implications for retail investors. One issue is that the best investment decisions are sometimes made when you go against the crowd (zig when everyone zags investment principles). For instance, a few years ago there were a ton of negative articles coming out about Apple ahead of a product announcement. That product was the iPad. If I used this product, then i would have been led to dump Apple stock.</li><li>I think that the speed of the stock market would make this sentiment tool somewhat useless since language, and sentiments towards companies can fluctuate and be quite subjective.</li><li>I think the quality control needs to be really good, or else you need a lot of input in order to have a fairly accurate result.</li><li>Getting enough workers to go through the list of links</li><li>I am confused as to how this is differentiated from the similar projects that you listed. In addition, building an accurate classifier may be a daunting task (as seen from our gun-article classifier in class).</li><li>You mentioned a lot of the problems in your presentation. There are soo many articles and companies, and it's not always intuitive what they would mean for the cash flows of companies. Also, I don't really understand how sentiment analysis will help with cash flows? If anything it should be for stock prices</li><li>It seems like the team still needs to come up with a good idea on how to aggregate the data and use it in a meaningful way. In the video, the team just mentions that they will perform some sort of analysis that will later be decided.</li><li>The volume of articles (you guys mentioned this) could become a problem -- in costs to pay the crowd and crafting precise predictive algorithms based on your sentiment analysis data</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: niub
  project: What Should I Cook?
  suggestions: <ul><li>I think it'd be a good idea to allow for a list of ingredients to be added in lieu of photos.  This would give the cook the option to cut down latency and make the process much more streamlined.  While this does take out the crowdsourcing aspect of the application, cooking and finding recipes isn't done regularly enough to require a crowd's help.  I'd scratch this idea!</li><li>A suggestion I have is to ask users whether the ingredients were correctly interpreted, so you can have a better understanding of whether the classifier is working or not.</li><li>If the image recognition portion of the project does not work as effectively as you hope, perhaps you could focus more heavily on customized recipes provided by users. This way you could capitalized on the crowd and provide users with suggestions they might not find on other recipe generation sites.</li><li>You could have an option for users to check off if they they have a decent number of spices/seasonings on hand, so that those items don't have to go through image processing and also don't show up as a missing ingredient during the recipe search.</li><li>Perhaps if the user keeps a running inventory of the items in her fridge, then this could work much faster. She would only have to take pictures when she gets new food items.</li><li>Focus the increasing the speed of getting the results. Hopefully also use human generated data to improve the existing recipe database.</li><li>This sounds pretty good as is, but if you had time you could refine the recipes for different tastes or dietary restrictions.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 1<p>
  excitement: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 2<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 5<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>The main issue with this idea is that latency will keep people from using the application.  Most people want to be in and out of the kitchen in as little time as possible.  If they have to wait five or ten minutes for someone to a) recognize all of the food in their refrigerator and b) send them recipes that they can hopefully make, I don't think that it will be as useful as it sounds on paper.  It's more likely that the cook will just make do with the ingredients he does have than spend time looking for a recipe he isn't familiar with.</li><li>The main problem I foresee with this project is that the classifier could be highly inaccurate, which would make the entire project fall apart.</li><li>As you mentioned in your answers, the most difficult portion of the project is the image recognition. The problem is that if you are unsuccessful in implementing this portion of the project, you have a service which is undifferentiated from several other recipe suggestion services.</li><li>It might take a while to get the responses you need, so it would only be useful if planning the meal far ahead of time.</li><li>Often, people have their food items scattered all around their kitchen (multiple cupboards, shelves, refridgerator), so uploading and sifting through multiple pictures for one meal could be taken to be more trouble than it's worth.</li><li>I know that at a previous PennApps Hackathon, a team put together a mobile app like this that doesn't even use crowdsourcing, just straight-up image recognition. So crowdsourcing would seem to introduce an unnecessary complication and expense.<p>On the usability side, I don't really see how this would naturally fit in with a user's cooking process. Would she have to take pictures of every single item in her kitchen? This also plays into the problem of latency. How long will it take to accurately verify the crowd's responses? A hungry, busy person doesn't want to wait a half an hour to figure out what to cook. This could easily be replaced with just manually entering all of her items into the system.</li><li>The search API and recipe database sound really cool. The part that utilizes crowdsourcing is also very clear. The problem might be the technology part you are using. How complete is the database and is it persuasive enough for the users to take a picture instead of typing down the items, which will drastically lower the cost and speed of the search.</li><li>As was mentioned in the description, it might be difficult to use an image recognition API and train it with a classifier.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: niub
  project: DesignMind
  suggestions: <ul><li>To help maintain the quality of the reviews, it would be a good idea to have some obviously bad logos, and if the reviewer upvotes that design, the rest of their reviews would be tossed aside. Quality assurance is a big problem with reviews, and with something as important as a logo, companies would want to know that they're getting real feedback.</li><li>One suggestion I have on improving this project is to give the crowd an option to type in a suggestion for a logo if they do not like any of the ones shown.</li><li>I would suggest providing an additional level of incentives for contributors. Perhaps they could be paid extra if they voted for the winning design. This way you might hope to see participants provide more earnest opinions.</li><li>You might want to control for the backgrounds of the crowd workers.</li><li>Perhaps throw in some test questions in which there is a clear right or wrong answer (i.e. what logo is best for a food company? show three logos and only one of them is remotely food-related).</li><li>I would drop the description of being the opposite of 99designs because it kind of seems like the same thing, except with a larger crowd and a voting mechanism.</li><li>The A/B testing idea is cool. But hopefully you can have some innovation on the interface, or a more complex measurement matrix. A direct voting system might not be attractive to the customers.</li><li>Maybe you could have different groups of people vote for different logos, so the company has more of an idea who their logo appeals to.</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 0<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Judging logos is a difficult task.  Due to its subjective nature, ensuring the quality of each review will be tough.  Workers could just randomly up- or down-vote certain designs and it'd be pretty tough to distinguish their opinion from random choice.</li><li>The main problem I foresee with this project is that the crowd could begin to spam / script to get more money.</li><li>It seems that it would be difficult to guarantee that crowdworkers are voting based upon their true opinions, and not simply trying to answer questions as quickly as possible. Since answers are subjective, it will be difficult to control for this issue.</li><li>The crowd might come from a limited set of backgrounds.</li><li>Quality control on whether the crowd workers are just randomly choosing logos or are putting thought into them might be hard to implement.</li><li>It will be very difficult to get people in the crowd to commit a lot of time thinking about a company to try to design a logo. Even if it does, there is hardly a guarantee that they will be any good because they are being sent out to the largest crowd possible of MT and CF. Also, the interests of the crowd expressed through the voting mechanism is not necessarily correlated with the interests of the company.</li><li>Why don't the customers just create tasks directly on Mechanical turk? I don't see how particularly you are increasing the existing platform through your app.</li><li>It might get expensive to get feedback on lots of logos.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 1<p>OK&#58; 4<p>Fair&#58; 2<p>Poor&#58; 0<p>
-
  group: niub
  project: ShopForMe
  suggestions: <ul><li>Instead of a personal judgement tool, I think this idea could be flipped around to help clothing companies dump their less attractive designs.  For example, the crowd workers would go through each article of clothing on the company website and either up- or down-vote it.  The company would then look at this data and decide whether or not they want to keep the products on the lower end of the spectrum.</li><li>One suggestion I have on improving this project is to use a hundred suggestions rather than a thousand. This should cost roughly a dollar, which is not too much for the user, who will still be getting a good amount of suggestions.</li><li>This cost issue could be addressed by incentivizing participation in other ways. Perhaps opinions providers could earn credits by providing answers to user questions. These credits could then be used to pose their own questions at reduced cost or free of cost.</li><li>Focus on how to speed up the process.</li><li>Maybe give crowd workers the option to suggest clothing articles that would go well with the ones they are evaluating to make a complete outfit</li><li>The use cases need to be a lot more specific and targeted. I don't totally understand how someone would use the app.</li><li>Select users from a fashion community who might have interests in providing fashion advice. Also create a peer review system to ensure the quality of the advices.</li><li>Maybe instead of finding the most attractive clothes, you can have different descriptors for the clothes like feminine, colorful, edgy etc, and therefore people can take those suggestions and figure out which they want to get from there.</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 4<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>To start, it could be difficult to get a consensus among crowd workers.  The problem lies in the type of person judging the clothing.  It could be a 15-year-old high schooler in Iowa or a 23-year-old in Miami.  Style is very geographically relevant, so looking nice in San Francisco isn't the same as looking nice in New York.  Moreover, people who care about fashion know what they like, and people who don't care wouldn't use the app because, well, they don't care.  This is a very niche market of people--those who care about what they look like, but have no sense of fashion--that are more likely to start reading GQ or Vogue than sending pictures of clothing to random people.</li><li>The main problem I foresee with this project is that a lot of hate could be generated by the crowd with mean comments.</li><li>As mentioned in the answers to the provided questions, cost seems to be a limiting factor here. A large number of responses will be desired in order to get to a more valid majority opinion and weed out unhelpful or unusual responses.</li><li>It might take longer to get a sufficient number of responses than would be ideal.  When shopping, you really need essentially instant feedback, you don't want to have to wait in the stores for people to vote.</li><li>It can be hard to compare clothing without seeing it on the person, but people will likely be hesitant to post pictures of themselves on sites like CrowdFlower or MTurk.</li><li>There is no description of quality control. Crowd workers can very easily go through designs and arbitrarily select one or the other.<p>The whole idea needs to be flushed out a lot more. Do users upload two photos of articles of clothing? Do they submit links? Doesn't the clothing matter a lot on who is wearing it? A dress that looks great on one person could look terrible on another.<p>Also, the goal of getting thousands of judgements for each submission is extremely ambitious and would be very costly. Perhaps this should be scaled down a bit.</li><li>I have seen a funny article before about people helping a guy preparing what to wear for his date online. I'm not sure however whether there is a broad need for this and how to incentivize the crowd. Moreover, it's hard to set a standard to reward/punish the crowds, thus hard to maintain the platform.</li><li>It might be difficult to control for quality, the workers really don't have an incentive to vote for the best looking clothes, and that might be difficult to control for as well, because people have different tastes.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 1<p>
-
  group: danicab,ibajekal
  project: Enterovirus D68 tracker
  suggestions: <ul><li>Get more people on your team! &#58;)</li><li>I can't think of how you'd fix the problems I mentioned above.</li><li>I would suggest looking more into how much location data you guys can actually obtain through tweets/Facebook posts. I would also suggest looking more into methods for statistically analyzing the spread of diseases. These steps will help you guys form a better understanding of the feasibility of the project.</li><li>There are existing APIs to pull data from social network. A possible detour for the first problem I mentioned above is search for related articles and extract geo-location information.</li><li>There might be research into this already in social media and disease spread - there may be algorithims that can help trending topics that you could use.</li><li>I tweets can be geotagged so you might not even need crowd workers for that</li><li>- Maybe have the workers do tasks that aren't related to data processing but can help the project where NLP and ML fail</li></ul><p>
  feasiblity: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Not all Tweets have geolocation data. Even a Tweet with geolocation data - that might not necessarily mean the person with the virus is at that zipcode.</li><li>While I feel that collecting Twitter data relating to the disease may not be too hard, Facebook data is probably another story, given that a lot of people do not make their posts available to the public at large. Also, there may be issues in identifying where the Tweets came from after they are determine to be relevant to actual cases.</li><li>I feel that it may be difficult to find the location associated with many of the tweets. Furthermore, even after obtaining results from the crowd workers, I imagine that it would still be very difficult to accurately determine how the disease is spreading.</li><li>You need geo-location information of the data to track the spreading. I'm not sure such information is always provides on the social network. <p>I'm also not sure about how prevalent the news about disease spreading are on the Internet. Is there authority always keeping track of this?</li><li>How do you separate trending topics to actual symptoms of a disease? Imagine it is very difficult to separate</li><li>Do people with Enterovirus D68 post about it on twitter? You should look up the hashtag and make sure there is enough useful data.<p>Getting facebook posts would not really be feasible due to FB's privacy settings in its API.</li><li>- Workers would be helping you by categorizing items, you'd still have to be responsible for implementing a form of ML in order to make sense of the output data<p>- There doesn't seem to be a need for the actual workers in this case, since all of the information they're providing can be extracted through automated tasks</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: danicab,ibajekal
  project: Priming Workers
  suggestions: <ul><li>Is there any way you can use the same data to make other sort of social-science conclusions other than priming?</li><li>Maybe increase the incentives for completing the second half of the survey as opposed to the first half in order to keep quality and motivation high for the responses.</li><li>Especially since this is a research oriented project, I would suggest defining the goal and approach to the project more clearly.</li><li>The economic theory is cool. But there might be some ways to better measure the effects and make comparisons between biased and unbiased results. Such techniques should be able to be implemented on existing crowdsourcing platform.</li><li>Give the users results and feedback about how they did to a control group.</li><li>I think you could make the project more interesting by using questions from pre-existing surveys or studies and comparing their results with your results (both from the primed group and the control group).</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 7<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 1<p>OK&#58; 6<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It's annoying to finish a long survey. <p>What if you don't end up yielding any new/real information about priming?</li><li>Since this experiment requires the crowd to answer both parts of the survey in order to get valid results, they may run into issues with getting the crowd workers to stick with their tasks the whole way through. Also, there may be the confounding factor of workers getting bored towards the end and answering the questions willy-nilly, thus eliminating any priming effect that may have occurred.</li><li>I feel that, without significant control on the experiment, it may be difficult to conclude what exactly was/was not caused by priming exercises.</li><li>Since this test is very subjective, while it may provide useful results, it will generate a lot of noises. It's also a bit hard to quantify the biases and utilize the results found in other settings.</li><li>Where will the priming exercises come from? Will need a lot of different exercises to test with the crowd, and need to dedicate time making the exercises.</li><li>I think this project is pretty straightforward so there wouldn't be many major problems in implementing it.</li><li>- Crowdflower workers don't represent a standardized sample, might incur noise and bias<p>- The experiment isn't double blind, so cases of observer bias can alter the results<p>- How will you prove that the questions are actually showing effects of priming? Who makes the questions?</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: danicab,ibajekal
  project: The Prisoners' Dilemma
  suggestions: <ul><li>Give context to all questions - you and your friend you've known your entire life, you and your friends boyfriend, you and your friends exgirlfriend, you and your ex who you're still friends with, you and your mom, etc...</li><li>Maybe to further differentiate the community and non-community groups, you could prime the non-community workers with prompts that deal with independence before they make their choices.</li><li>Since this is a research oriented project, I would suggest formalizing the process by which you plan to analyze the data. I would also suggest thinking of ways in which you can more concretely formulate conclusions from the data.</li><li>Maybe based on the original prisoner's dilemma problem, you can implement more dimensions in the decision process thus finding some new insights.</li><li>Is this research related to determining if crowdworkers are aware of a larger community? How does this harness the wisdom of crowds?</li><li>Are you framing the prisoners dilemma question in terms that relate to crowdflower or using the classic scenario? If you just present it as a hypothetical situation (You and another Turker are arrested and imprisoned...) the results are not as meaningful as if you make the worker think that their choice will have an effect. MTurk allows you to reward users with bonuses so you could have the workers actually play the game and potentially win/lose.</li><li>- Maybe implement an actual zero-sum game that asks workers to cooperate for a payout (real incentive driving results)</li></ul><p>
  feasiblity: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 0<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 5<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 7<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I don't understand why a sense of loyalty would exist among CrowdFlower workers.</li><li>I guess the main issue I foresee has to do with dividing the group into CrowdFlower workers who feel a sense of community and workers that do not feel this sense of community. While trying to a elicit a community-oriented feel through prompts and questions may be somewhat effective, I'm still under the impression that, because this is a factor that your experiment turns upon, it may prove to be a confounding variable.</li><li>How will you test for the effect of loyalty on the prisoners dillemma when you're simultaneously also testing for the very existence of that loyalty among crowd flower workers?</li><li>I feel that it may be difficult to statistically conclude the extent to which CrowdFlower workers feel loyalty to one another. It also might be difficult to statistically conclude the extent to which you guys can influence workers' perception of loyalty.</li><li>I feel similar experiments have been done with the crowds. I'm not sure the research will yield any particularly new findings.</li><li>Quality control, since the answers are subjective, not necessarily right or wrong.</li><li>Whether this effectively tests TPD depends on if the workers were only answering questions or whether their choices had outcomes.</li><li>- Metric of measuring success is very subjective<p>- The experiment has a predefined motive (we want to show workers that community is better)<p>- Measuring the existence of community among workers has more facets than asking them to self-report</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: deveshd,katmill,sheffers,sierray
  project: MassterMix
  suggestions: <ul><li>Have a review system for djs so that the bad djs will be weeded out.</li><li>I think it might be worthwhile to cut down on the work that is necessary of the crowd by inserting an intermediate step between the user's initial 3-5 songs and a completed playlist. Perhaps a list of songs could be automatically generated (like Pandora) based on the initial 3-5 songs. The crowd could then pick songs from this list, and the songs that the crowd chooses are passed along to the end user to be part of the playlist.</li><li>I think if you change the incentive from money to a type of point system that allows you to create more playlists by suggesting more songs to people, you're likely to have a more ideal crowd of music enthusiasts rather than people looking to spam. People can get extremely passionate about music, and I think you're better off making it a community type project where everyone is on there because they love music and want to explore new songs and show other people new songs rather than for monetary incentive.</li><li>Maybe allow people to tag songs with particular key words and dynamically create playlists of that key word</li><li>Maybe have professional listeners who listen to an entire playlist before it is given to the user to determine whether it is satisfactory.</li><li>I think most people - even if not music aficionados - could identify songs that fit with certain moods. What about if you curated a list of 300 songs and played clips and had them categorize them into categories you wrote, instead of letting the user come up with a category that could be super vague.</li><li>1. You should also create a ranking system and personal profile for each user and give them extra incentive by giving star DJ and other titles to people whose playlists are mostly accepted. If you could create a sense of community on there (like SoundCloud) and a platform for people to get invited to be real DJs, you are likely to drive more traffic<p>2. Game like interface on mobile end? People can open the app and it will pop up short clips of two songs and ask you to rate which one is more hippop/Christmas/etc</li><li>For testing, you could always target one genre, mood, or  type of event to craft a playlist for. If you can get proof of concept that the crowd DJs on MassterMix a spot-on playlist of songs for that particular scenario, it would be grounds to take this further.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There might not be enough djs to fulfill all the requests or the dj suggestions might not be up to par to what the requestors want.</li><li>The main problem I see in the project is sourcing a knowledgeable crowd. Even on a small scale, it would be difficult to source strangers and guarantee that they are knowledgeable about music. Perhaps monetary incentive will attract knowledgeable participants, but there is a significant hurdle to clear before this incentive plan could be effective.</li><li>The biggest problems for this project are language barriers and a variety in musical tastes, along with the fact that, given monetary incentive, people may just spam people with songs, or promote their own songs, etc.</li><li>It is hard to get the crowd simulation for this project idea. How is this different from Spotify music suggestions algorithms?</li><li>People might have very different ideas of what song does/does not belong in an existing playlist.</li><li>Music curation is really hard even for people - you can play them a few tracks and have them organize where they go - but I think any thing more curated will be difficult (for example, choosing a song with no curse words). Also everyone has very different music taste - so how do we quality control in a way that is fair to the crowdworker?<p>I think music aficionados are fairly hard to come by and it will be difficult to guarantee the songs they choose fit in with the vision.</li><li>1. People have very different tastes of music and it might be hard to ask a complete stranger to recommend music for another person (on the other hand, if this stranger is actually a friend from Facebook who you know likes the same music as you do or a great DJ, sure)<p>2. Copyright issues - how do you get permission to play those songs?</li><li>Cash Flow -- are people paying to be part of this network? Because you're paying the djs. Are you planning to build algorithms to handle the picking without paid labor eventually?</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: deveshd,katmill,sheffers,sierray
  project: Critic Critic
  suggestions: <ul><li>Asking detailed questions about each article to make sure the information is the most detailed and accurate.</li><li>I really like this idea! I don't have many suggestions, only to be careful about the way articles are chosen to be given to crowdworkers, as I mentioned above.</li><li>Upon compiling the dictionaries, it may be a good idea to weed out all of the words that are used commonly throughout each group in order to find the words that are used specifically in articles about a politician of a certain demographic.</li><li>Think about alternative ways to generate keywords</li><li>Maybe use the crowd again, this time to determine if there are different types of bias.</li><li>Possibly provide them a wikipedia page of the subject in order to give them somewhere to dive into to find demographic information and parse out the subject more clearly (sometimes it's not just one).</li><li>After the first round of asking the crowd to determine demographic, it might be more helpful to use the crowd again instead of a machine learning algorithm - ask the crowd to rate the general impression they get about this politician based on reading the article, instead of finding common words with a classifier, and see whether the crowd's answers are common within a demographic group</li><li>Use Google's stopword dictionaries to do a lot of pre-processing before you extract feature words. https://code.google.com/p/stop-words/<p>Try crawl a wide distribution of sources for articles.<p>It might make your project more focused & feasible to examine one time period or a single election and the candidates.</li></ul><p>
  feasiblity: Excellent&#58; 8<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 8<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There could be a low precision problem like the gun classier assignment.</li><li>I agree completely with this team in that they will likely run into issues with crowdworkers being unable to identifying the subject of articles. This will create an additional challenge for the team, as they may have to focus their efforts on providing crowdworkers with articles with a singular, easily identifiable subject.</li><li>Many articles will likely mention multiple politicians which will make it harder for the workers to determine who the subject of the article is in those situations. It's also hard to determine what comprehensible keywords for each group that are useful for analysis might consist of. Moreover, although there may be some words that are used more often among some demographics than others, since the articles are all about politics, the most commonly used words for each group will likely still be political.</li><li>How do you obtain the sample set about inconsistencies in representation?</li><li>The data compiled in the dictionary might not be useful for analyzing anything about race/gender bias. Even removing stopwords could still result in unusable data.</li><li>This is great! <p>A problem I could forsee is people being unable to identify demographics - in countries where race is not as diverse as in the US. Ages are also difficult - in the US politics are really transparent, but in other countries this is not always the case.<p>Also, I do think it'll be difficult to tell if the crowdworkers have done a good job at this.</li><li>1. It might be hard to compile a collection of articles just about individual politicians - many articles are about a political incident/political commentary, and to classify out only those that focus on a single politician requires a lot of work in building the classifier<p>2. Hard to find commonality between articles about politicians from the same demographic - they might be portrayed in the same way, but not necessarily using the same words (the political/media vocabulary can be extremely diverse), so using machine learning to find frequently used words might not give you too much information</li><li>Thoroughly extracting stopwords from the articles so that all of the feature sentiment words you extract are meaningful.<p>There could be a big bias depending on where you crawl for articles.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: deveshd,katmill,sheffers,sierray
  project: EbolAware
  suggestions: <ul><li>Having turkers answer more questions about the tweets to be able to have more meaningful conclusions.</li><li>I would suggest implementing a mechanism which limits the sources from which tweets are pulled. Such a mechanism might allow you to find a more diverse range of responses, rather than the humor-skewed responses I speculate you will receive. Perhaps you could source tweets from only verified Twitter accounts, thus hopefully filtering out a large number of humor tweets.</li><li>I think if you had a range of emotions available for the worker to choose from, with an intensity rating next to each, you could compile more telling data.</li><li>Maybe also gather other sources in addition to twitter</li><li>Perhaps you could sort the tweets by country, and then only have crowdworkers from that country analyze emotion to get a better sense on the true content of a tweet.</li><li>I think that intensity will be really difficult to measure - but maybe just having a three rating system - thumbs up for intensity level, and thumbs down for chill might result in more clear results. Magnitude will be difficult but could be done in a second round and you could line the intense tweets up and have them rank them. <p>Rating really can only be done successfully if you give them alternative options.</li><li>Instead of just looking at how people think about Ebola,it might be more helpful to gather data on how people's reaction to Ebola changed over time and vary by geographical area. It will be also helpful to get some more collections than Ebola (disease that happens in the US, disease that happens in countries we have more interaction with, terrorist attack, etc)</li><li>It could be important to track demographics based on geographic region for this -- are you only looking at the buzz about Ebola in the US? There are going to be a lot of confounding variables effecting the relationship between individual response and global response, and I would make sure you outline them meticulously.</li></ul><p>
  feasiblity: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 5<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Categories might not be specific enough to result in valuable conclusions.</li><li>The most significant problem I see with this project is trivial results. I foresee a large portion of tweets being categorized as humor or as lacking any easily identifiable emotional response. This will make it difficult to gather any significant findings.</li><li>I think rating the emotion and intensity of tweets is extremely subjective. For example I could feel extremely scared about something, or extremely sad about something, but those are still two different emotions. Also, the definition of success for this project has little to do with emotions but rather mentions recurring themes, and I don't see where that connection was made. Moreover, it's not necessarily easy for a worker to judge a person's emotions based on tweets, many of which are likely to be tweets about articles about ebola rather than an individuals personal sentiment.</li><li>How do you handle retweets? What if a tweet doesn't meet one of the listed emotions?</li><li>People are often sarcastic in tweets - this could be interpreted as genuine emotion by non-native english speakers. How will you account for this?</li><li>It's think that intensity is hard to read into unless you're really really fluent and native English speaker. Also rating that intensity is even harder - one person's 2 could be another person's 7, and both could genuinely have put what they believe. I think QA will be really difficult on this.</li><li>It will be hard to get the Turkers to rate the emotion of a tweet based on standardized guidelines and guarantee quality control.</li><li>Honestly, none besides what you mentioned! I think it's pretty straightforward</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 1<p>OK&#58; 5<p>Fair&#58; 0<p>Poor&#58; 1<p>
-
  group: agarwalv,etha,rohanb,shreshth
  project: DressedUp!
  suggestions: <ul><li>Have the user include a brief description of the product to make it easier to match it with a business product.</li><li>Think about an easy way to convey price comparisons. You may also want to incorporate product reviews similar to Amazon. You mentioned vendors would like this for data analytics; I think this has potential but needs to be clarified.</li><li>The incentives for users to take pictures of articles of clothing should be reconsidered</li><li>I can't think of any suggestions, maybe just implement this for students first to help scale it down.</li><li>Maybe give the people who have answered other people's posts more priority when they post their own inquiries in order to further incentivize people to participate.</li><li>It may be best to simply attract users to your site, and they can be lead to the relevant sites where companies showcase their clothing. Eventually, businesses can be brought in as participants. Your idea is cool, but I find it very unlikely that you could create a 2-sided market right away, especially given the time frame.<p>Still a really cool idea! I would probably use it just for the product recognition from photos.</li><li>A machine learning based algorithm which can do image analysis and link to vendors such as Macy's or Target or Amazon to link you directly to the item if found. If not, wait for crowdsourced results.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 1<p>OK&#58; 4<p>Fair&#58; 2<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>The quality deals more with businesses that might provide the answers -- what about users? Also, not sure how it is with guys' shoes, but it's generally easy to figure out where girls get their shoes from (or so it seems). Shoes are trending a lot, and a large variety of people have the same shoe.</li><li>How will you match the pictures of the users to products that businesses can sell? Will payment and delivery be included on the platform or will it be done through the business's website? It could be difficult getting businesses and users to sign up initially.</li><li>The problems that you have identified seem to be pretty comprehensive, and I agree with them.</li><li>It may be difficult to simulate vendors.</li><li>I think that a person interested in particular piece of clothing that someone else is wearing could just as easily ask the person wearing the item than take a picture of it.</li><li>Possible privacy issues? Taking pictures of people without their knowledge/consent could be a problem.</li><li>Vendors on the site may try to push items that are similar to what users saw but not exactly the same in order to boost revenues. Also, there does not seem to be a clear incentive for non-merchants to post where to find the clothing items.</li><li>I find it unlikely that businesses would actively particpate on your website. Even small companies would not be interested until you get a sizeable user-base.</li><li>It may involve some serious stalking to get your desired fashion accessory.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: agarwalv,etha,rohanb,shreshth
  project: Planet Photo
  suggestions: <ul><li>Allow photographers to add descriptions to pictures so that it is more helpful for users when planning trips.</li><li>Aggregate photos already posted elsewhere on the web to create more exhaustive database.</li><li>Check out Noun Project to see how they do licensing of their images.</li><li>Perhaps try to think of ways to distinguish this project more from google images/flickr. I wasn't completely sure how it is different from existing projects.</li><li>Maybe have an option on each photo that allows viewers to flag photographs that are not relevant to the site in order to prevent spam.</li><li>Instead of using photographers, have the crowd source pictures from any source including google etc and then have the pictures link back. The project could be converted to a picture ensemble or album tool like 8tracks does for music.</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 3<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  problems: <ul><li>It seems as though the amount of contributors can easily outweigh the number of viewers. Also, what is the incentive for viewers to use your site? There are a huge variety of places to view images on the web.</li><li>How will you get initial photographers to sign up? Will there be ways to prevent photographers from just up voting their own photos? How does this differ from Google's similar project?</li><li>You may want to consider offering stronger incentives for the photographers, if, for example, they don't really care about publicizing their work.</li><li>Gaining sufficient photographers addressing interesting places worldwide may be difficult. The viewers may also just want to use google images instead.</li><li>Ensuring that photographs cannot be downloaded without being licensed by users, and also ensuring that photographers simply do not post photos taken from google images</li><li>I wasn't able to find this in your description but, are photographers just choosing a location to tag in some sort of menu? Or is there some way that it is automatically set using gps? Because if they are manually tagging location then I think this could lead to people tagging their pictures for more exotic locations such as Bora Bora when in fact the picture was taken in Florida just to generate more traffic to their pictures.</li><li>In terms of quality control, you did not mention how you would deal with the issue of people potentially uploading photos that are irrelevant to the location that they post to. Also, as was brought up during the Q&A session, new photos that are added will have to somehow be integrated in so that they do not always show up at the bottom of the search results.</li><li>The main problem would simply be getting people to start using it.</li><li>Photographers wouldn't usually want to upload their best pics for free. It would probably become a bit of a marketing tool. Also passive advertising could become a contentious issue.</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: agarwalv,etha,rohanb,shreshth
  project: Füd-Füd
  suggestions: <ul><li>More details need to be added to ensure that an Eater will always pay for their food, and that Runners are trustworthy (won't tamper with food).</li><li>Payment may be difficult to set up in the project time constraint. Perhaps make it so that payment is just settled directly between the runner and eater.</li><li>The benefits for eaters and runners makes sense, but seems hard to implement. You may want to try to make the platform for a particular set of popular restaurants to increase the probability of a matching.</li><li>None that I can think of at the moment - just make sure to research postmates fully to make sure you provide a different sort of value-added service.</li><li>Some way to further incentivize the delivery men - maybe they get subsidized/free meals from the restaurant or something? Not really sure how that would be made feasible but it's a possibility.</li><li>Maybe add in a map functionality so that people are matched with each other if the food they desire isn't the exact same restaurant, but the two restaurants are right next to each other (e.g. Chipotle and BBP).</li><li>Make sure that this isn't something that has been done before.</li><li>Adding a location based criteria would help the matching process. A method to agree upon the price needs to be formulated.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There may be a problem with peak hours of the day, where there tends to be a higher rate of food consumption -- there will be a high amount of Eaters, and potentially a low amount of Runners. Also, what happens if there's frequently no one who wants to be a Runner?</li><li>Setting up the payment system could be difficult. How will you get early adopters to test your project? Is there a way to alter an order after it is made?</li><li>I agree with the problems that you stated in your questionnaire</li><li>It is really hard to set up the crowd of eaters and runners. To function properly, you need a balance of eaters and runners. Creating a matching between the eaters and runners may not be as frequent as anticipated. What would eaters do if they don't get a matching, but they have already been waiting for a while?</li><li>A viable competitor may already exist - see postmates&#58; https://postmates.com/.  Also, Grub-Hub already delivers from most places around campus so i don't think that this app would greatly expand eating options</li><li>I think that it might have a real supply/demand imbalance. While I can see a lot of people wanting to get food delivered to them, I don't see a lot of people wanting to walk to an extra place (the house of the person they are delivering to) for a very small monetary compensation. Which is why I think that the crowd might be much smaller than anticipated.</li><li>If the supply of runners is low compared to the supply of eaters, there may be situations in which the runner is unable to carry back all the food requested by him/herself (e.g. if he goes to a pizza place and a bunch of people want him to bring them back a pizza).</li><li>It may be hard find a price where people would be willing to pay to get their food delivered and others would be willing to deliver. Also, do you guys know about Seamless? At a quick glance, seamless does the same thing.</li><li>The problem is that often runners and eaters won't be in the same place and the number of available runners may be affected by the fact that the eaters location.</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: PReTweet
  suggestions: <ul><li>Perhaps when the tweets/social media posts are given to crowd, a brief description of the company and a description of what the company is trying to project itself as is needed.</li><li>Possibly implement a machine learning algorithm to predict the emotion of tweets, trained and confirmed by crowdflower workers. For an interesting experiment, see if Turkers from different countries find the same tweet as expressing different emotions.</li><li>Have the user input some information about their typical follower so that crowdworkers may be able to better understand the intended audience. Include test questions with clear right or wrong answers that may have nothing to do with the rest of the project but would help ensure quality.</li><li>I think you have to sacrifice a little bit of quality in order to make the process faster. If I have to wait an hour or more to hear back about whether or not a tweet is appropriate, I'm unlikely to do that. But if only 3 workers look at it and I get the OK within a few minutes, it's something that could feasibly be used.</li><li>It may be advisable to create a list of types of inappropriate tweets and do some general crowdworker training, as opposed to assuming all workers know by default what is appropriate or inappropriate. It also depends on the company.</li><li>Going off of the problem I mentioned previously, I would suggest trying to implement a mechanism that is able to only assign workers of a company's target audience to analyze that company's tweets.</li><li>1. Maybe focus more on the rating of tweets and their sentiments (e.g. funny) and sell that information rather than trying to screen them for offensive content</li><li>I really like this idea</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Your team has two very similar ideas.<p>What is considered funny to one worker might not be funny to another.<p>The same worker could fill out form many times.Make sure that is not allowed.</li><li>The different cultures and backgrounds of crowdworkers can influence how they see a tweet and skew the results. It could be difficult to get users who want to check their tweets before since most people who post controversial tweets feel strongly about the subject and act on impulse rather than processed thought. It could cost a lot if every single tweet is paid for.</li><li>Checking attributes of a tweet such as if it is funny may depend on the age or demographic of the user. For example, the followers of a 16 year old may find very different tweets funny than the followers of a 50 year old. Quality control could also be difficult as some of the questions may be subjective.</li><li>The crowd isn't really representative of a specific person's followers, so it won't say much about how well liked a certain tweet will be. I think this does work pretty well in terms of the workers filtering out inappropriate tweets. In fact, this is probably a better version of social scrub because it catches the tweets before they're posted rather than after. However, using pretweet would prevent tweeters from posting instantly, which, if I'm not mistaken, is a common use of twitter.</li><li>Funding the project. <p>Also, the metrics used to determine if a tweet is inappropriate. For example, while some tweets may be very culturally or otherwise insensitive, it's possible that the CrowdWorker community might not be receptive to picking out that kind of insensitivity.</li><li>The primary problem is that opinions on what is considered humorous naturally differ. The project mentions gaining a diverse crowd to get diverse opinions, but I believe it would probably be better to somehow filter the crowd to be people who are close to a particular company's target audience since those are the opinions that the company cares about.</li><li>1. Latency -- companies may want to publish their tweets immediately and not wait for workers<p>2. What's appropriate may depend on the company, sarcasm, etc <p>3. Why couldn't this be done in-house?</li><li>Possibly not getting a representative cross-section of people could lead to errors.  In terms of humor, speed would be important, because someone may tweet the same joke before yours was approved.</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: CrowdCheck
  suggestions: <ul><li>Lessen the scope of this project.</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 1<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I might just skip over things anyway - who says the crowdworkers will really pull out things that i'm willing to use my time reading?<p>Is making a Chrome plugin that doable?</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: Social Scrub - Cleansing Your Employees' Social Media Profiles
  suggestions: <ul><li>Perhaps when the tweets/social media posts are given to crowd, a brief description of the company and a description of what the company is trying to project itself as is needed.</li><li>The appropriateness or inappropriateness of a tweet could be evaluated on a continuous scale instead of only 2 choices. Have a continuous monitoring system so that employers don't have to manually choose to run the algorithm every week or whenever but are notified automatically of inappropriate tweets (a bit encroaching though). Expand to image sharing like Instagram.</li><li>Consider using the crowd to find an initial list of offensive words. Including initial questions on gender and race may help to check that the crowd is somewhat diverse. Also, perhaps there are specific topics that a company wants employees to avoid talking about. Adding a feature that limits the search of offensive tweets to these topics could limit the amount of false positives.</li><li>This wasn't explicitly stated, but I would certainly remove any names or references to the company from tweets. However, even if this happens, a worker can easily copy and paste the content of a tweet into google and find whose twitter it was from since the project is based on employees public profiles.</li><li>I would maybe suggest instead of the current setup, which is first using machine learning to flag tweets then using the crowd to check the algorithm, perhaps first use the crowd to flag tweets and use said flagged tweets to train the machine learning algorithm.</li><li>None</li><li>1. Try to define a better standard for what appropriate is, it might vary from company to company</li><li>If anything, the classifier should catch too many posts, and that way there will be false positives and very few false negatives, so the crowd can filter the false positives and the service will catch almost all the bad posts.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 2<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 8<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>There can millions of social media posts about a company.<p>This is a large amount of data.Writing code for the ML algorithm might be a challenge.<p>Also , what can be considered inappropriate varies person to person.<p>I don't know If the crowd knows what the company's marketing team is trying to project the company as.</li><li>The main one is how the group will make the machine learning algorithm accurate enough so that companies will be willing to use it (I would expect it to be very high). This could be difficult just because of the nature of machine learning and the limited data to work with since tweets are generally short. Also, how is the group going to consider what is inappropriate, especially in the eyes of an employer? Some companies are stricter than others.</li><li>How do you ensure that a diverse crowd reads each tweet (for some tweets, only a small subset of the population of people may the information offensive)? Similarly, places such as Planned Parenthood provide services that they think reflect positively on themselves; however, some crowd workers may not feel the same way. How can these conflicts be avoided? Lastly, will this only check for content that employees have on public profiles? If employees post offensive content on a private Twitter account, this could still reach many people and reflect badly on the company.</li><li>Obviously there's the problem that what may be deemed inappropriate from the company's point of view wouldn't necessarily be deemed appropriate by workers in the crowd, and vice versa. Moreover, I think it's highly unlikely that any company would be comfortable using random people to search their employees for inappropriate tweets. The whole point of having this done within a company is that the poster takes down the tweet and hopefully very few people in the public see it. Having all your employees social media accounts thoroughly examined by the public seems a little counterintuitive.</li><li>Mostly just that, in order for this to be a viable business idea (which it is, and I like it!), you have to be quite careful and make sure it works.</li><li>Although, as mentioned in the description, many large companies do this internally, is it actually legal?</li><li>1. Privacy concerns, employees might get upset <p>2. Employees could make their social media private in response</li><li>The classifier may be an issue, but the crowd should be able to correct the false positives.</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: The Weighing of the Bull
  suggestions: <ul><li>Ensure one user gets only one opinion.Somehow find a way to use the data, otherwise its just money being spent on crowd flower workers.</li><li>Have separate tasks for different factors being examined so the amount of learning can be cut down per task. After the learning, have some sort of test to make sure workers went through the material.</li><li>Consider limiting the stocks valued to well known brands so that workers don't need to waste time learning the basic business model of the company (unless you want to see if more obscure industries are valued lower in the crowd than the actual market). It may be better to ask college students who have some understanding of economics value the stocks instead of random crowdworkers; however, you would have to compensate college students at a higher rate.</li><li>I think it's necessary to ask the Turkers questions about the video as the video is being played, or, better yet, post random numbers throughout the video that the Turker must record in order to show that he's watched the video.</li><li>It could be interesting/useful to partner with some research group that would find this information helpful, so they could either a) fund you b) give you guidance as to where to take this project.</li><li>There have definitely been similar projects that utilize Twitter instead of the Mechanical Turk crowd to obtain sentiments, so I would recommend looking at those projects for implementation and analysis ideas.</li><li>1. You could offer this as a service to investors who pay, and use that money to fund the crowd</li><li>Perhaps focus on a certain stock or small set of stocks, so you will get a larger crowd per stock, and thus hopefully better data.</li></ul><p>
  feasiblity: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 8<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 3<p>Good&#58; 6<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 2<p>OK&#58; 5<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 6<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>As a social experiment it is a great idea.Is there any way to make money from this?<p>Just asking people on crowd flower may be a very small set also.<p>The same user on crowd flower might just fill out the form many times to make money.</li><li>The initial learning workers have to do is extensive and could be a factor to reduce number of workers that perform the job. The exact factors that are being studied are not clearly explained in the description. It would probably be better to have a certain goal to start out with so analysis can be more productive.</li><li>How can you ensure that crowd workers will actually learn the economics and do the work to research these companies? Some industries of these stocks are somewhat esoteric and could be difficult for the average worker to value.</li><li>These HITs would be fairly time extensive, and I know that point will be countered by paying the workers more, but I'm assuming you want a fairly large crowd for your data, so I could see this project becoming pretty expensive to implement. It will be interesting to see what price HIT you will have to use in order to get workers to spend the time actually doing it. Moreover, it's entirely feasible that I worker could not pay any attention to the economics video and just randomly guess for his predictions.</li><li>I think this project is really interesting, but requires providing your crowdworkers with quite a lot of financial incentive (especially if they have to brush up on the stock market beforehand). How would you obtain this money?</li><li>I think the main problem is not being able to identify any statistically significant results, but this is also part the goal of the project, so I wouldn't necessarily describe it as a problem; maybe more of a disappointing outcome.<p>The other problem is that people may potentially be forming opinions about companies they know nothing about except for a few articles, so it may be difficult to get informed opinions.</li><li>1. Will be tough to weed out bad responses since they are all opinions <p>2. Need a metric for success</li><li>Funding may be an issue if you plan to pay a decent wage, as many articles will need to be analyzed.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: MarketChatter
  suggestions: <ul><li>I'm pretty sure the crowd in this experiment is MTurk analyzing sentiment, not Tweeters and journalists.</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I don't understand - are you using an algorithm for sentiment analysis or turkers for sentiment analysis? There was very little info.</li></ul><p>
  quality: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: carjack,cswords,elibrock,noashpak
  project: DealFlow
  suggestions: <ul><li>Use an idea that does not require a crowd of such expertise.</li></ul><p>
  feasiblity: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 0<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Sorry, I don't get the VC business at all. Wouldn't it be really really hard to get this crowd? Don't people with this sort of expertise of better things to do?</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: fahim,fmelp,mechanic
  project: Note Turker
  suggestions: <ul><li>Keep the same workers with the same lecture, to keep time costs down, with respect to watching/listening to a lecture. This way successive rounds cut down on overhead. (this may lead to more problems, though).<p>The project seems very well thought out.</li><li>Some kind of initial test to identify good Turkers for the job. Potentially try to reduce number of rounds of Turkers. Create an easy user interface for uploading and receiving final notes. Establish objective criteria for Turkers to rate the quality of notes.</li><li>A suggestion I have on improving this project is to possibly include other students rather than CrowdFlower workers. If a student from the class either missed the lecture or wants to review it, they could be the ones to transcribe the whole thing and you would not have to pay them anything.</li><li>Aggregation and steps look well thought out. Maybe some other form of quality control could be useful. Would it be possible to get someone with more of a background in the subject to check?</li><li>1. It might be a more practical idea to have Turkers take notes of existing online open courses from major universities. In that way you won't need to worry about whether people will actually submit videos of lectures they missed, but can start on videos we can find online and build up a nice portfolio of notes for the project<p>2. If you could add a small test in the beginning to find Turkers with some relative background of the lecture, that would be great</li><li>You could break the videos up by subject and offer them to users based on their own background. Let them choose videos that they're confident they will be able to understand.</li></ul><p>
  feasiblity: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 5<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 2<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 8<p>Good&#58; 0<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>I agree that expertise may play a role in the value of the crowd's transcription. For example, if I wasn't familiar with math, would I interpret n choose k properly, and be able to notate it? This is easily extendable to other disciplines, especially where not so common words come up, where it's linguistically harder to parse words that you might not know or be familiar with in that context. <p>If people do have experience, though, what if that causes them to include biases in their note taking?<p>The payment will have to be compensatory for the amount of time involved, which may end up being too much for the notes output.</li><li>The quality of the transcriptions might be a problem if screens for Turkers are not done. How many Turkers would have to be paid for transcribing one set of notes? If the Turkers are not familiar with the field they are taking notes for, especially for higher level courses, the notes may not be a good reflection of the lecture. Each Turker also may have different styles of taking notes. If a screen is performed, it might be a hassle to set up for each different subject.</li><li>The main problem I foresee with this project is how the videos will be aggregated for the workers to take notes on if the videos require you to be a student to access them.</li><li>I agree with the problems that you mentioned in your video</li><li>Advanced courses would probably not work with this idea, but this may be very useful for introductory courses. Also, where is the video coming from? Is it easy to get a recording of the class?</li><li>Getting workers who understand the lecture. Also, lecture notes will not always be plain text.</li><li>1. For users who are unable to attend their lectures, they usually won't have the video of the lecture to upload in the first place (even if they do, it's usually restricted to students and can't be just uploaded to MTurk as a link), so the initial resources to start the project might be scarce<p>2. As the team already mentioned, some Turkers might not be able to understand the lecture (which I think is very important in good note taking) due to fragmentation of the material and lack of background knowledge</li><li>As you mentioned in your project description, it may be hard to find quality note takers because these people may have no background in the subject at hand.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: fahim,fmelp,mechanic
  project: MySelfie
  suggestions: <ul><li>Make sure the crowd is somewhat experienced with varying social media, so they understand what different underlying social structures are inherent in choosing a photo. <p>Have successive rounds so you can diminish the effects of subjectivity as best as you can.<p>Maybe have a list of labels describing if a set of pictures are better than any other set for certain settings (again, Facebook profile picture, vs professional picture, vs dating site picture etc)</li><li>Use a test or set some requirements to ensure that the users are doing the job properly. Possibly ask users where they are from so workers from that country could be used to avoid cultural differences. Offer the option of feedback about lighting or background settings for workers to explain their preferences.</li><li>A suggestion I have on improving this project is to ask the user to leave a review about whether the picture that the crowd chose helped them or not. It would be nice if you could find a way of quantitatively measuring this.</li><li>It may be useful to restrict who can rate your picture by certain parameters. I don't know how that would be put into effect.</li><li>Since it is mentioned that the project will incorporate opinions from different culture/ethnicity/age/professions, etc, it might be helpful to take into consideration what type the person who posted the picture is looking for and get more targeted opinions</li><li>Ask turkers to provide comments on why they liked or disliked a picture. That way the user will have an understanding of where the results came from and they may see something in a picture that they didn't think about before.</li></ul><p>
  feasiblity: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 6<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 4<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 7<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 7<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It definitely is subjective with respect to attractiveness. It is similar to the example shown in class where crowds rank the cuteness of cats. <p>You might run into the problem of the crown not understanding the social minutia involved in picking a dating website picture. Does it follow the same criteria as say a professional picture or a Facebook picture, etc.</li><li>There is a concern of getting enough users that will post their photos and have them judged on crowdflower since people generally do not appreciate being judged. As mentioned by the group, the workers are skewed and this could alter the results based on cultural differences. Some quality control to prevent spammers would also be helpful to reduce costs.</li><li>One problem I foresee with this project is that there is no way to test whether the crowd is answering properly or not. People may begin to order the pictures randomly to get paid.</li><li>As you mentioned in the video, the subjectivity of looks can be a potential issue, and there may not be a clear consensus on what the hierarchy of photos is.</li><li>Will people be willing to pay money for this type of service? What stops them from just posting pictures on social media and seeing the response they get from others.</li><li>1. Privacy concerns - how many people will be willing to share 10 of their own pictures with the anonymous crowd?<p>2. Different people have very different taste, and the average suggestions from the crowd might not be the most helpful ones</li><li>People are sometimes very self-critical, especially the type of person who would use this type of service. They may receive the results and say to themselves, well, I still don't agree with this. This picture is ugly because of x,y,z...</li></ul><p>
  quality: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: fahim,fmelp,mechanic
  project: PictureThis
  suggestions: <ul><li>Perhaps cross picture book stories from different cultures - this would get a more novel approach to the answers, and might also add an interesting cultural variable to the experiment.<p>Again, this seems very thought out already.</li><li>A control to check for plagiarism could be implemented. The amount of description suggested could be calculated based on the length of the initial book. A reading level of each set of pictures could also be told to the workers.</li><li>A suggestion I have to improve this project is to cut back on some of the objectives and just focus on one or two rather than three.</li><li>If the idea is to test users' creativity maybe the project should be scaled down to normal pictures. The key being finding a set of data that is ready to go, as opposed to building a new data set from scratch.</li><li>1. To obtain the book material, it might be helpful to form a partnership with Nook or Kindle to use some of their children's books collection for the experiment<p>2. How exactly this experiment can further serve research in social sciences (child psychology/creative writing) can be more fleshed out</li><li>Focus on turkers who have some connection to the age group in question (i.e. parents, teachers, children's book writers, etc.) This will promote higher and more tailored quality in the results.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 1<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 5<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 3<p>Good&#58; 1<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>It would be easy for certain workers to take the story in a completely different direction relatively quickly, whether that's a good thing or bad thing.<p>People may be familiar with these stories and could just copy what they originally said.</li><li>It is possible, especially if the books chosen are popular, that the workers will have read the books previously and either consciously or unconsciously be influenced by the actual content. There needs to be some way to check for plagiarism, unintentional or not, and related to the book or others. Limiting the descriptions to 1-2 sentences per picture may be too little to create a substantial story, especially if the original is written with a higher ratio.</li><li>The main problem I foresee with this project is that using this many tasks could begin to become quite expensive.</li><li>The aggregation and quality control methods need some additional thought</li><li>The effort of finding the children's books and also the the size of the overall project seem to be the biggest issues.</li><li>I think it will be difficult for workers to understand how to screenshot the illustration.</li><li>1. Collecting the original book material with pictures might be hard<p>2. The crowd might have very different understanding of pictures and texts compared to children to whom the book was originally designed for</li><li>You may run into trouble when you receive stories that are just not fit for a child of the age group you'r targeting. The language or story may be too complex or inappropriate if the turker doesn't understand the target demographic.</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
-
  group: paarth,salbert,subbindu,whitecar
  project: Rent-a-Kitchen
  suggestions: <ul><li>Having more incentives for people to rent out their kitchen.</li><li>I believe that some sort of limitation should be put in place (signing up with your penn email). If users of the app are limited to the Penn community for now, then you would guarantee some level of similarity between the users that would alleviate a lot of concerns about safety, and incompatible pairings. In addition to using this app to use a kitchen when one is not available, Penn students could also use it as a social app that allows you to meet new people, go to their house, and cook them dinner. <p>In turning this app slightly more social, consider looking at an app called Grouper. It brings together three guys and three girls, and gets them to meet at a bar for a free drink. What happens for the rest of the night is up to them. This app could be pivoted to be slightly more similar where two groups of three Penn students are paired to meet at a kitchen, cook and enjoy a dinner together.<p>I believe that a rating system needs to be more clearly specified. If I provided a kitchen for a seeker who happened to be very hygienic in the kitchen, was friendly, did not cause a mess or a racket, but happened to be a terrible cook, then that should be made clear on the rating. It could be that someone else would not mind a mess if they got a great meal, and someone would not be interested in the meal, but would want a seeker that is known to be clean and quiet.</li><li>Somehow connect Facebook data to limit it to Facebook friends or friends of friends. I'd feel much more comfortable letting a personal connection into my room to use my kitchen. <p>Also, force contact information to be shared for liability safety concerns.</li><li>I think this is only feasible in a college setting, so maybe loosening some of the restrictions for quality control, and relying on the fact that college students are trustworthy to their fellow students to ensure people feel safe going to another person's kitchen. Use images and reviews to ensure kitchen cleanliness, etc. rather than checking everything beforehand.</li><li>Maybe if you could connect people who are friends/friends of friends? This would make it less likely to be complete strangers.</li><li>Allow options for kitchen requests for people with particular dietary restrictions</li><li>I don't think the demand for kitchens is actually that high in a larger scale - for example, most college students even with kitchens - don't cook because it takes too much time and is a pain to clean up (particularly on Penn's campus, only freshmen don't have kitchens). I think this would have a hard time scaling - maybe if you extended this more to lending spaces (like for example, party space as well) it might be more useful for the average college student.</li><li>Cool idea, but I'm not sure how feasible the execution is. If you were to pursue this, I would suggest thinking more about the implementation details.</li><li>It could maybe be made more social, with the host and guest cooking together or eating together.</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 5<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 1<p>Good&#58; 0<p>OK&#58; 7<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 1<p>Good&#58; 3<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 1<p>
  relevance: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Kitchen renters might not be incentivized enough to rent it out as it requires them to be in there room during the use of the kitchen and some kinds of food can cause odors that people might not like.</li><li>In terms of getting people to use the service, I imagine there would be a high level of awkwardness associated with it. Even if some sort of demographic system is implemented so that Penn students can interact with other Penn students only (or some sort of filter), both sides may have different expectations of the social norms associated with this interaction. I believe adding filters or filling out a survey could be crucial to enhance the state-of-the-art pairing program.<p>I am unclear on how FDA approval for home-cooked meals works, but I imagine that there would be an issue with getting this approval or generally ensuring the safety of the meal. <p>As mentioned on the proposal, there also safety issues related to letting strangers into your home, etc. Cleaning up after one's self may be another concern.</li><li>Not sure if people will sign up to let other people use their kitchens. I wouldn't let a stranger just into my room to use my kitchen<p>How in the world can you ensure food providers to make only FDA approved foods?<p>I don't really see this project really utilizing crowd sourcing.. I don't see people going back and rating the kitchen.</li><li>I think the biggest issue is quality control. I'm not sure how you can ensure the quality of food (I don't think FDA approval is feasible), so it's probably best to just stick with monetary compensation if food quality is a concern. I'm also not sure it's feasible to audit all potential kitchen providers beforehand since you'd hopefully want a pretty big crowd.</li><li>The most difficult thing will be finding people who will be willing to rent out their kitchen, even with incentives.</li><li>How do you simulate the crowd? Why wouldn't the kitchen seekers rather just cook at a common kitchen in residential building or at a friend's place?</li><li>1. Getting people to actually give up their kitchens, finding a market will be more difficult and therefore being able to see how well things are working in real life will be extremely difficult I think. Attracting the crowd will definitely be really hard.<p>2. This is a cool idea but probably doesn't have that in depth of a dive into crowdsourcing for a final project (because it will be difficult to see the crowd which is dependent on real people willing to give up their kitchens).</li><li>Who will audit the kitchens? How will the food be FDA approved? I'm not sure Penn is an appropriate target audience because almost all, if not all, of the living spaces have personal kitchens or community kitchens.</li><li>None, really. I really love this idea!</li></ul><p>
  quality: Excellent&#58; 1<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 2<p>Poor&#58; 0<p>
-
  group: paarth,salbert,subbindu,whitecar
  project: Note My Time, Note My Problem
  suggestions: <ul><li>Forcing the student to sign up for a slot each lecture so that it does not encourage people to skip lectures which can affect their notes and understanding of the course.</li><li>It may be beneficial to have users select who they would like in their group (maybe certain students prefer to work with people they know, etc). <p>I think you might want to rethink the way slots are assigned or split up. It may be more efficient to have one note taker continue past the allotted time if the same topic is still being discussed. It could be a good idea to have the interface be similar to google docs, with an icon indicating who the current note taker is. Notes can be reviewed in real time, and the note taker has the ability to pass to someone else. Passing would mean that the note taker gives the responsibility onto someone else, who gets an alert and must accept in order for the current notetaker to change. Additionally, a non-active notetaker can ping the current notetaker to request to be passed to. The app/website should monitor how long each note taker on a group was active for, and can highlight the notes using different colors to see the quality of the notes as well. In this way, all members of the group can clearly see each individual's contribution, and a layer of flexibility can be added so that notetakers can take notes only at the times it is most convenient for them.</li><li>My understanding of crowdsourcing is more like - Hey, I wasn't able to really take notes during lecture today. Can you all share your notes from this section of the class when I wasn't taking notes? I'll decide which was the best notes and share those notes back with all of you.</li><li>Split note taking on a per-class basis rather than at different points throughout the class. Add more of a social networking interface as well. I think what could set this apart from a shared google doc with friends and make it more relevant to the class is the crowd aspect, so you can do this in classes where you may not necessarily have friends, and you can leverage the crowd, in this case your class, to find study groups, make review sheets, etc.</li><li>This is an interesting idea, however students usually show little interest to class/notes until midterm/final season. Maybe a similar idea can be implemented for Midterm/Final Review. This may increase participation by a lot.</li><li>Allow uploads for notes on readings outside of class time</li><li>Possibly everyone taking notes for the full time is better - or switching lectures instead of 20 minute intervals would be much more incentivizing for the students (potentially so they don't have to come to class &#58;P). <p>It's hard to get high level concepts in 20 minutes and just makes things complicated. I think it would be hard to get notes that flow by 20 different people in a single lecture.</li><li>One way to immediately and automatically filter freeloaders is to prevent people from viewing the notes from a given class until they have contributed notes of their own.</li><li>Enforce a common note taking platform. I wouldn't like to use such a service is some users submitted typed out notes, some handwritten, some on lecture slides.<p>If a google doc kind of system could be used, every user could know that notes are being taken and don't have to find out that someone messed up only when it could be too late.</li></ul><p>
  feasiblity: Excellent&#58; 8<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 8<p>Good&#58; 1<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 3<p>Fair&#58; 1<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Because the lecture time is split up, some students taking notes will only pay attention for their time slot and not the entire lecture. This can affect the quality of their notes and might even discourage students to pay attention to the entire lecture.</li><li>The main concern I have with the note taking process is that some people may prefer to take detailed notes that leave very little content out, while others are perfectly content with writing keywords and remembering the rest. I believe that over time, a group will manage to figure out how to collectively improve their note taking. This would probably best be achieved through some sort of forum that enables the group to establish guidelines for how the notes should be, and to give suggestions so that the group becomes more compatible. This may also be highly beneficial for groups that may have several mutual classes in the future because once they have a working system in place, they can continue to efficiently use it in future classes. <p>I think the interface and UI of this project would heavily impact its use. If it is very user friendly and somewhat gamified, then there will be a positive impact on the quality and commitment of the crowd. Additionally, a system can be implemented where the person in a group gets a certain number of points for the effectiveness and amount of notes they contribute to a particular project. These points can then be traded in for access to a different class's notes.</li><li>I don't really get what this has to do with crowdsourcing. Is a bunch of people signing up to take notes really crowdsourcing?<p>How does being given a 20 minute time slot solve the problem of not wanting to take notes?<p>How can you ensure that you have enough people that exactly two people are covering every 20 minutes for every class..?? Isn't it a huge problem if you're counting on these notes and those notes happen to be particularly bad for a certain section?</li><li>I think you might have trouble recruiting a crowd for this since it seems like a little more complex than just sharing a google doc with others in a class and collectively taking notes. I think there also might be some issues with the final product if different users are taking notes for just short bursts of time (do you have to be watching the clock all class for when it's your turn?)</li><li>Recovering from poor note-takers may be difficult.</li><li>What is the incentive for people to use this product rather than other existing note sharing sites like scribd?</li><li>1. I think that taking notes for only 20 minutes of the class will result in a lot of lost information - what does 20 minutes? how do we prevent a few minutes from being cut out during the transition period? More importantly, taking notes is more than just writing what the teacher says - it's highlighting concepts. I'm worried students, knowing they only have to do their 20 minute share - will not pay attention to the other parts and miss thc core. <p>2. I also think it's a bit binary - students take notes also to help remember things better - either students take notes or they don't becuase it doesn't help that much and i think few students would like to only take a few notes.</li><li>One potential problem is that if a group is relying on one person for notes at a given time and that person misses his/her turn, the rest of the group suffers permanently. It may be beneficial to look into sharing/aggregating notes between groups.</li><li>Number of users&#58; If there are only 2 people sharing a class on the website, they can't be assured only 10 minute note taking slots.<p>Lots of students wouldn't be comfortable with notes from other people.</li></ul><p>
  quality: Excellent&#58; 4<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: paarth,salbert,subbindu,whitecar
  project: The Lazy Analyst
  suggestions: <ul><li>I don't think only the top 10 companies should be returned. I think it will be helpful to have all of the top companies returned so that the analyst can review all the top companies as determined by the workers, catching mistakes or things the workers could have missed.</li><li>The pitch mainly discusses the aggregation of this data to build some sort of database, but does not really discuss what should be done with the database once it is populated. I think it would be a good idea to have the info organized in a website that is accessible to analysts, summer interns, and individual investors. These users can search any company they want to receive the crowd sourced list, can rate or comment on existing lists, or contribute fresh lists. <p>This second step can help treat the problems mentioned above that relate to the quality and aggregation of the data. In order to incentivize analysts to contribute, the website can have a monthly subscription fee that can be circumvented using points. These points are allocated according to an algorithm that takes into account the level of contribution, the quality of the contribution, and how many other people are benefitting from this contribution. In this way, if an analyst has already performed an analysis on a company, he would be willing to share the information he got out of his research in exchange for information he will need later. This will save time for analysts and help build a database that would be highly useful.</li><li>Allow for customization - maybe I'm looking for 20 similar companies, not 10.</li><li>Target to a crowd of students and partner with a company that will both have the resources to compensate students (who have pre-existing knowledge and also more free time). Partnering with something like Bloomberg will also help with quality checking and dissemination of information once it is acquired. You could also introduce a rating system that students could provide to potential employers, demonstrating their abilities to pick out similar companies.</li><li>Is there a way to make users choose out of pre-selected companies rather than a company of their own.</li><li>Allow filter for suggesting what metric the comps are based on (i.e. EV/sales, EV/EBITDA, EV/EBIT, P/E). This is important since different industries require different valuation multiples.</li><li>This is a good idea! Have you concerned doing a sort of machine learning thing with tech articles - often techcrunch etc will have competitors listed within the company article and might be reasonable to parse out, especially because they usually have Crunchbase links. You might get a better correlation than an API because it's real journalism.</li><li>If the initial algorithm is able to look up comparable companies, are the crowd workers necessary? Consider, if you haven't already, just using the crowd workers to validate the results of a machine learning algorithm for identifying comparable companies so that in the long-term the crowd workers may not be necessary.</li><li>The tasks are pretty big and I'd assume that you'd have to pay workers a significant amount to get better results. Also financial API could be expensive, so maybe charge the users?</li></ul><p>
  feasiblity: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 4<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 4<p>Good&#58; 5<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 5<p>OK&#58; 4<p>Fair&#58; 0<p>Poor&#58; 0<p>
  recruitment: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Having unreliable workers can cause faulty results.</li><li>It may be difficult to attract the quality workers of a good enough level to complete the HITs, but after some trial and error with how the HIT is described, priced, etc. I am sure that the group could be able to target the correct group of crowd workers for this project. <p>The aggregation stage may be difficult to implement. I would imagine that every company would have a few fairly obvious comparable companies that all crowd workers will agree on, however there may be a couple of highly obscure ones that only one or two crowd workers find, that end up being perfect results. For this reason, I imagine that the group would have to experiment with different formats of how the turkers can submit their findings until they find the interface that guarantees higher quality.</li><li>What's the financial API you mention? You trust it to accurately give you results? Have you ever used it before? Can you be sure that there exists 10 similar companies in the 75?</li><li>Generating a large enough, quality crowd may be difficult. I think a worker will need a decent amount of background knowledge or will have to do some research while working on each hit, which will either mean increased costs since the job is harder, or having a small crowd of only qualified workers. It also may be difficult to incentivize qualified workers since people with this background information are probably the ones who would be using it too (bankers, for example) who have limited time.</li><li>This seems like a lot of work for the crowd(outside research). Most crowd workers don't show this much interest in individual tasks.</li><li>This doesn't readily allow private company valuation. After you get the multiple using comps, you need the private company's financial information on which to apply the median comp multiple. This does work for public companies.</li><li>1. I think a financial API will probably not return that many similar companies with similar business models - so 10-15 seems like a really high number to sort through.<p>2. I think similar companies don't necessarily have the same business model at all - but just happen to be tackling similar topics so that distinction should definitely made.</li><li>The main issue will be accuracy of results, especially since the average layman might not have the ideal skillset.</li><li>Very simple idea and could be really cool. I'm a but skeptical about the quality of crowd predictions.<p>Difficult to get analysts to actually try it out to see how well it works.</li></ul><p>
  quality: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: dhrupadb,jran,kongjih,pleamy
  project: Shoptimum
  suggestions: <ul><li>Multiple images for the same celeb. Perhaps another  a crowdsourcing element that does find the exact same brand.This would help identify images of the exact product giving clearer images.<p>For example, if it is a Hermes Bag, first identifying the exact bad, pulling up those images and then finding cheaper variants might be a better idea.</li><li>You could include a wider array of stores to cross-reference for prices, as to attract the widest array of users. <p>You could give the crowd a diagnostic for certain clothing items and their names, to make sure they could accurately identify and describe them.</li><li>Try cutting back on some of the work that the crowd workers have to do. May lead to better overall performance and more flexibility in operation and scalability.</li><li>You may have to cut down on the amount of steps for the project. Although they do ensure quality results, it will take many crowdmembers and potentially a lot of money to do all the steps. However, people may do this just for fun depending on the audience?</li><li>I think there are too many steps in this project, which decreases the chances of success since you're asking for so much from the crowd. If you could somehow pare down the necessary involvement, it might be helpful.</li><li>I think it is a cool idea, but some details in terms of how the crowd will exactly participate in the process should be thought about a little more.</li><li>Having multiple options, so displaying the top 2 or 3 results for the user. Also allowing users to make requests.</li><li>I think this project looks great, maybe a bit more of a concrete definition of a successful project would be good.</li><li>Can people contribute their own photos that they want to be analyzed? I'd love to be able to find out where to buy the clothing I see in random online photos.</li></ul><p>
  feasiblity: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 0<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 1<p>Poor&#58; 1<p>
  excitement: Excellent&#58; 5<p>Good&#58; 1<p>OK&#58; 1<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 2<p>Good&#58; 6<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Great Idea, Measuring success could be more well defined.<p>Also, it might be hard to implement when people want something very similar to what the celeb is wearing, say something like a watch.<p>For example just saying steel watch is not enough.</li><li>The different meanings of affordable could potentially offput some users - i.e. while there's a huge difference between celebrity expensive and Macy's expensive, Macy's could still be more than some people want to spend on certain items (like a gray scarf, where at Macy's it might $25-30, but other places $5-15). <p>The crowd will need to know potentially more than basic clothing to parse out certain aspects of fashion. Descriptors that could be useful could be omitted, like the difference between a cardigan and a sweater, or saying skirt instead of a-line skirt, etc.</li><li>Different workers may have varying different tastes in 'choosing the best' style - leading to difficulties in aggregation.</li><li>This is a very well-thought out idea, but it does require many components. It may take a lot of resources to fulfill all the steps of the project, especially since the Crowd is used for multiple steps.</li><li>Crowd incentive—how will you accrue finances to financially incentive people?</li><li>I am not entirely clear on the aggregation method and the quality assurance method.</li><li>As they mentioned, the possibility of clothes being out of stock and it not being updated.</li><li>I foresee a potential with funding, as the presenters have mentioned.  Also, I feel it may be possibly overambitious.  Querying all of those photos and the sites like Amazon and Macy's would involve heavy use of API's which will take time to set up and integrate, if they exist at all.  However, if those are not an issue, then this project would be very feasible.</li><li>Would people really want to tag pictures? I think people would be incentivized enough to contribute to the other crowd-sourced parts.</li></ul><p>
  quality: Excellent&#58; 5<p>Good&#58; 3<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: dhrupadb,jran,kongjih,pleamy
  project: MuralMapPhilly
  suggestions: <ul><li>Involve government tourism, they could sponsor perhaps.This would encourage people to join if they are being paid.</li><li>Perhaps use geolocated data from pictures people have already taken of these murals to construct your mural map. To combat some of the problems incentivizing the crowd, you could make it more like a competition to tag these murals - maybe get together with the City of Philadelphia Mural Arts Project and make a city-wide competition with prizes and stuff.</li><li>The two round system of crowd workers is a very good validation technique and is a good crowdsourcing model!</li><li>Maybe you could add even more features like comments/opinions about different murals -- almost like a forum.</li><li>I think this project should be thought over a little more carefully in terms of who the crowd is, how the crowd will be incentivized, and how the entire project will be turned into an interactive, easily-indexed library of Philadelphia Murals. It might be good to partner with the Mural Arts program, one of the mural-based courses at Penn, or a mural-based community program to locate crowdworkers. It would be also really great to have some sort of search functioh that can find murals by keyword, description, painter, title, location, etc., without physically clicking on the map.</li><li>I would add an incentive for the crowd, and potentially expand beyond murals to other forms of art such as sculptures, statues, etc. Of course, if the project is successful, you could expand to other cities as well.</li><li>Since the incentive is for the greater good, making the interface as easy to use will be really important for the crowd. Overall, I think it's a great idea though.</li><li>I would provide more details on how to incentivize the crowd, e.g. if you will pay people to post the images and check the accuracy.</li></ul><p>
  feasiblity: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  incentives: Excellent&#58; 2<p>Good&#58; 3<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 1<p>
  aggregation: Excellent&#58; 5<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 0<p>Good&#58; 3<p>OK&#58; 4<p>Fair&#58; 1<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 0<p>Good&#58; 6<p>OK&#58; 2<p>Fair&#58; 1<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 5<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>Actually getting people to post on location of murals,also how will you measure success?<p>Another thing, could you generate a way of making money from this?</li><li>I think that finding a crowd interested enough in each city to do this might be difficult. This of course depends on the delivery of crowdsourcing - if it was similar to the common tagging apps that people have, then it might not be so hard. Incentivization might be a hard problem to overcome as well.</li><li>Not sure how easy it would be to incentivize people to participate for this semester; however, I may be underestimating the number of people who visit murals. As mentioned, it may be hard to find people knowledgeable about murals.</li><li>I'm confused about the implementation of the project—will it be done on Google Maps, or some entirely separate website created just for this purpose? If the entire project is just done on Google Maps, I imagine that would make it very difficult sort through murals by name or artist. <p>Also, the idea of using CrowdWorkers to verify the murals is probably not all that feasible since I don't think a) most CrowdWorkers live in Philadelphia b) would be willing to physically go to the mural and verify it (since they can't verify it online, which is the whole point of this project).</li><li>There is not a clear plan for incentivizing the crowd. Also, I am not sure of the frequency with which new murals are painted, so this idea may quickly lose its interactive, crowdsourcing element once all murals are added to the map.</li><li>The incentive for the crowd is for the greater good, so it might be difficult to collect a lot of data. Also getting name and artist may be more difficult information to obtain.</li><li>There may not be enough people who fit your description of the crowd, those knowledgeable of Philadelphia area murals.</li><li>I am not sure that people really need this project. Is there really nothing online about public artwork in Philly? If not, then are there known specific groups who would like to have this database? Basically, you need to make sure that you can find some people who would contribute.<p>Also, would there be people on Mechanical Turk who are knowledgeable enough to contribute? Especially given that an assumption of this project is that this information is not readily available online.</li></ul><p>
  quality: Excellent&#58; 7<p>Good&#58; 2<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 0<p>
-
  group: dhrupadb,jran,kongjih,pleamy
  project: Read it Out
  suggestions: <ul><li>Perhaps ensure only one member does one book.<p>Come up with a way to ensure quality and accuracy better, beyond just simple rating systems.</li><li>When doing the second round crowd correction, perhaps lead them in with a few sentences prior to the sentences they're judging to give them context.<p>Scaling back the project to something more manageable; narrowing it's scope.</li><li>Possibly find a way to make readings more consistent from reader to reader. DIfferent emotions, inflections, and accents may greatly affect the reading of the audio book.</li><li>I would think about more applications for this project and develop the potential market. This will help tune your audience, which in turn can help develop incentives and the app itself.</li><li>To help with flow, retain the same readers by incentivising them with a bonus for good work.</li><li>Maybe implementing some sort of smoothing so the voices would sound like one fluid voice, but that's outside the scope of this class.</li><li>I would presume that the person who wants an audio recording could do their own quality control by listening to the audio produced. I don't think people will be willing to check the quality of other people in their spare time without more incentives (like money?). Something to consider.<p>Also, I wonder what software there is currently available to automatically improve the quality of human voice audio. If there is something out there, it would be nice to include.</li></ul><p>
  feasiblity: Excellent&#58; 4<p>Good&#58; 2<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 1<p>
  incentives: Excellent&#58; 4<p>Good&#58; 4<p>OK&#58; 0<p>Fair&#58; 0<p>Poor&#58; 1<p>
  aggregation: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  excitement: Excellent&#58; 2<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 1<p>
  recruitment: Excellent&#58; 3<p>Good&#58; 4<p>OK&#58; 2<p>Fair&#58; 0<p>Poor&#58; 0<p>
  relevance: Excellent&#58; 8<p>Good&#58; 0<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
  problems: <ul><li>If different people read different parts of the same book, the audiobook will sound very odd.Imagine if the original script is a play. Repeatedly voices will change giving a poor effect.<p>Also, say the person reading the book is not fluent in the language.<p>He /She may misinterpret things.<p>Authors of the book may have copyright issues.</li><li>Differences in language throughout any given book&#58; Is someone's name pronounced with a different accent or stress in the beginning of the book than the end (from one crowdworker to another)?<p>Books are long, the amount of books is vast, audiobooks exist.<p>End-users listening to a crowd-created audio book might get lost in its dynamics - the voice changing every paragraph or so, and with it, tone, style, clarity and intonation.</li><li>Different dialects in speech and cultural differences inherent in language styles and diction will make a crowdsourced audiobook very difficult to listen to and comprehend.</li><li>This is a really cool idea. Like you guys mentioned, the audio files might be too big to submit. Also, it might be a bit weird having different people in an audio book speaking for each paragraph -- but I guess that's just a preference.</li><li>While it's pretty cool to be able to crowdsource an audiobook, I'm concerned that many listeners would not appreciate an audiobook with several different voices—I know that I, myself, would find it kind of annoying. <p>I'm also not sure if people would want to contribute to this without a financial incentive. And, if you're providing payment, where would this funding come from? How would the application fund itself?<p>I also am not quite sure that there is a market for this application, so I would definitely look into doing some market research.</li><li>The main problem that I foresee is the aggregation method, and carefully designing the method by which recordings that did not pass the quality assurance tests are not compiled and sent back to the crowd to re-record. Also, I am not sure how large the tokens of text will be after the text is tokenized, but it may be undesirable to have the book being read by so many different people on the audio file.</li><li>There's the possibility that paragraphs P1 and P2 pass the quality check part, but then not sound good one after the other. Also some paragraphs could be much longer than others, so from the workers perspective, it's more work to read the longer paragraph and not worth the same as a much shorter paragraph.</li><li>I think the only problem would be funding the necessary amount of tasks to get quality audio.</li><li>Each level of quality will require (are people willing to pay for the audio, is this simply open source, etc.) might require different forms of quality ensurance.</li></ul><p>
  quality: Excellent&#58; 6<p>Good&#58; 2<p>OK&#58; 1<p>Fair&#58; 0<p>Poor&#58; 0<p>
